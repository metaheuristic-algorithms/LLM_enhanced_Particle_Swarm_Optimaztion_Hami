{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T05:01:55.254374Z",
     "iopub.status.busy": "2025-12-15T05:01:55.253629Z",
     "iopub.status.idle": "2025-12-15T05:16:35.769888Z",
     "shell.execute_reply": "2025-12-15T05:16:35.769140Z",
     "shell.execute_reply.started": "2025-12-15T05:01:55.254352Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLASSIFICATION TASK 1: LLM Controls c1/c2 + Particle History\n",
      "======================================================================\n",
      "  [Info] Using 5641 images (25.0%) for training.\n",
      "\n",
      "=== Task 1: LLM-Controlled PSO (c1/c2 + Particle History) ===\n",
      "    > Eval: L=1, F=16... Acc: 0.8663\n",
      "    > Eval: L=3, F=41... Acc: 0.8754\n",
      "    > Eval: L=1, F=30... Acc: 0.8607\n",
      "    > Eval: L=1, F=114... Acc: 0.7994\n",
      "    > Eval: L=1, F=43... Acc: 0.8126\n",
      "\n",
      "Iter 1/8 | Best Acc: 0.8754 | c1=2.00, c2=2.00\n",
      "    > Eval: L=4, F=55... Acc: 0.8189\n",
      "    > Eval: L=3, F=41... Acc: 0.8424\n",
      "    > Eval: L=3, F=52... Acc: 0.8520\n",
      "    > Eval: L=4, F=28... Acc: 0.8735\n",
      "    > Eval: L=1, F=43... Acc: 0.7867\n",
      "\n",
      "Iter 2/8 | Best Acc: 0.8754 | c1=2.00, c2=2.00\n",
      "    > Eval: L=4, F=18... Acc: 0.8723\n",
      "    > Eval: L=3, F=41... Acc: 0.8659\n",
      "    > Eval: L=3, F=42... Acc: 0.8687\n",
      "    > Eval: L=4, F=16... Acc: 0.8699\n",
      "    > Eval: L=1, F=43... Acc: 0.8173\n",
      "\n",
      "Iter 3/8 | Best Acc: 0.8754 | c1=2.00, c2=2.00\n",
      "  [LLM] Consulting for suggestions + c1/c2 tuning...\n",
      "  [LLM Analysis]: Velocities in filters dimension are large (e.g., -86.53, -45.96, +39.51), causing big jumps like F:115→28 or 56→18, while layers quickly hit the upper bound and stay there. This indicates overshooting rather than fine local search, not stagnation. Reducing c1 and c2 should dampen these swings and allow more controlled exploration around the best region (L≈3–4, F≈30–45).\n",
      "  [LLM] Updating: c1=2.00->1.60, c2=2.00->1.60\n",
      "  [LLM] Replacing P4 -> [3, 38]\n",
      "    > Eval: L=3, F=38... Acc: 0.8559\n",
      "  [LLM] Replacing P1 -> [4, 35]\n",
      "    > Eval: L=4, F=35... Acc: 0.8671\n",
      "    > Eval: L=4, F=16... Acc: 0.8496\n",
      "    > Eval: L=3, F=44... Acc: 0.8388\n",
      "    > Eval: L=3, F=33... Acc: 0.8727\n",
      "    > Eval: L=4, F=16... Acc: 0.8615\n",
      "    > Eval: L=1, F=39... Acc: 0.8440\n",
      "\n",
      "Iter 4/8 | Best Acc: 0.8754 | c1=1.60, c2=1.60\n",
      "    > Eval: L=3, F=35... Acc: 0.8384\n",
      "    > Eval: L=3, F=46... Acc: 0.8798\n",
      "    > Eval: L=3, F=42... Acc: 0.8293\n",
      "    > Eval: L=4, F=67... Acc: 0.8639\n",
      "    > Eval: L=1, F=40... Acc: 0.8540\n",
      "\n",
      "Iter 5/8 | Best Acc: 0.8798 | c1=1.60, c2=1.60\n",
      "    > Eval: L=3, F=47... Acc: 0.8802\n",
      "    > Eval: L=3, F=47... Acc: 0.8814\n",
      "    > Eval: L=4, F=37... Acc: 0.8281\n",
      "    > Eval: L=4, F=55... Acc: 0.8659\n",
      "    > Eval: L=1, F=40... Acc: 0.8743\n",
      "\n",
      "Iter 6/8 | Best Acc: 0.8814 | c1=1.60, c2=1.60\n",
      "  [LLM] Consulting for suggestions + c1/c2 tuning...\n",
      "  [LLM Analysis]: Velocities in filters dimension are relatively large (jumps of ~10–50 filters in one step, e.g., P3 from F=16→67 with +51.49; P0 from F=16→35 with +19.30), indicating overshooting around the best region (~F=46–48). Layers dimension shows small, controlled changes (±0.5 or less) and is already concentrated near L≈3.8–4.0, close to the best L=3. The swarm is not stagnating; it is exploring aggressively and oscillating around the good region rather than refining it. This suggests c1/c2 are slightly too high for fine-tuning and should be reduced to dampen velocities and allow more local search near [3, 46–48].\n",
      "  [LLM] Updating: c1=1.60->1.40, c2=1.60->1.40\n",
      "  [LLM] Replacing P2 -> [3, 47]\n",
      "    > Eval: L=3, F=47... Acc: 0.8739\n",
      "  [LLM] Replacing P3 -> [3, 45]\n",
      "    > Eval: L=3, F=45... Acc: 0.8747\n",
      "    > Eval: L=3, F=54... Acc: 0.8579\n",
      "    > Eval: L=3, F=48... Acc: 0.8743\n",
      "    > Eval: L=3, F=35... Acc: 0.8818\n",
      "    > Eval: L=3, F=33... Acc: 0.8762\n",
      "    > Eval: L=1, F=41... Acc: 0.7529\n",
      "\n",
      "Iter 7/8 | Best Acc: 0.8818 | c1=1.40, c2=1.40\n",
      "    > Eval: L=3, F=41... Acc: 0.8731\n",
      "    > Eval: L=3, F=31... Acc: 0.8627\n",
      "    > Eval: L=3, F=29... Acc: 0.8731\n",
      "    > Eval: L=3, F=28... Acc: 0.8735\n",
      "    > Eval: L=1, F=41... Acc: 0.8603\n",
      "\n",
      "Iter 8/8 | Best Acc: 0.8818 | c1=1.40, c2=1.40\n",
      "    > Eval: L=3, F=36... Acc: 0.8691\n",
      "    > Eval: L=3, F=38... Acc: 0.8790\n",
      "    > Eval: L=3, F=34... Acc: 0.8747\n",
      "    > Eval: L=3, F=36... Acc: 0.8536\n",
      "    > Eval: L=1, F=40... Acc: 0.5575\n",
      "\n",
      "======================================================================\n",
      "RESULT: Best Params = [L=3, F=35]\n",
      "        Best Accuracy = 88.18%\n",
      "        Time = 863.1s\n",
      "        Final c1=1.40, c2=1.40\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Classification_Task1_LLM_C1C2_Control.py\n",
    "# LLM controls c1, c2 parameters + provides particle position/velocity history\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    AVALAI_API_KEY = \"\"\n",
    "    AVALAI_BASE_URL = \"https://api.avalai.ir/v1/chat/completions\"\n",
    "    LLM_MODEL = \"gpt-5.1\"\n",
    "\n",
    "    DATASET_PATH = \"/kaggle/input/waste-classification-data/DATASET\"\n",
    "    IMG_SIZE = (128, 128)\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_CLASSES = 2\n",
    "    TRAIN_DATA_PERCENTAGE = 0.25\n",
    "\n",
    "    POPULATION_SIZE = 5\n",
    "    MAX_ITERATIONS = 8\n",
    "    \n",
    "    # Initial C1, C2 (LLM will update these)\n",
    "    C1_INIT = 2.0\n",
    "    C2_INIT = 2.0\n",
    "    \n",
    "    W_MAX = 0.9\n",
    "    W_MIN = 0.4\n",
    "\n",
    "    # Search Space\n",
    "    MIN_LAYERS = 1\n",
    "    MAX_LAYERS = 4\n",
    "    MIN_FILTERS = 16\n",
    "    MAX_FILTERS = 128\n",
    "\n",
    "    EVAL_EPOCHS = 1\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. ENHANCED MEMORY MODULE\n",
    "# ==========================================\n",
    "class OptimizationMemory:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.particle_states = []\n",
    "        self.c1_c2_history = []\n",
    "    \n",
    "    def add_record(self, params, acc):\n",
    "        self.history.append({'params': params, 'acc': acc})\n",
    "    \n",
    "    def add_particle_state(self, iteration, positions, velocities, fitness):\n",
    "        state = {\n",
    "            'iteration': iteration,\n",
    "            'particles': []\n",
    "        }\n",
    "        for i in range(len(positions)):\n",
    "            state['particles'].append({\n",
    "                'id': i,\n",
    "                'position': [round(positions[i][0], 2), round(positions[i][1], 2)],\n",
    "                'velocity': [round(velocities[i][0], 3), round(velocities[i][1], 3)],\n",
    "                'fitness': round(fitness[i], 4)\n",
    "            })\n",
    "        self.particle_states.append(state)\n",
    "        if len(self.particle_states) > 3:\n",
    "            self.particle_states.pop(0)\n",
    "    \n",
    "    def add_c1_c2_record(self, c1, c2, iteration):\n",
    "        self.c1_c2_history.append({'iter': iteration, 'c1': c1, 'c2': c2})\n",
    "    \n",
    "    def get_context_string(self):\n",
    "        if not self.history:\n",
    "            return \"No history yet.\"\n",
    "        sorted_hist = sorted(self.history, key=lambda x: x['acc'], reverse=True)\n",
    "        best_3 = sorted_hist[:3]\n",
    "        worst_3 = sorted_hist[-3:]\n",
    "        \n",
    "        context = \"=== OPTIMIZATION MEMORY ===\\n\"\n",
    "        context += \"TOP 3 Configurations (Highest Accuracy):\\n\"\n",
    "        for i, h in enumerate(best_3):\n",
    "            context += f\"  {i+1}. [L={h['params'][0]}, F={h['params'][1]}] -> Acc: {h['acc']:.4f}\\n\"\n",
    "        context += \"\\nWORST 3 Configurations:\\n\"\n",
    "        for i, h in enumerate(worst_3):\n",
    "            context += f\"  {i+1}. [L={h['params'][0]}, F={h['params'][1]}] -> Acc: {h['acc']:.4f}\\n\"\n",
    "        return context\n",
    "    \n",
    "    def get_particle_state_string(self):\n",
    "        if not self.particle_states:\n",
    "            return \"No particle state history.\"\n",
    "        context = \"\\n=== PARTICLE STATE HISTORY ===\\n\"\n",
    "        for state in self.particle_states:\n",
    "            context += f\"\\n--- Iteration {state['iteration']} ---\\n\"\n",
    "            for p in state['particles']:\n",
    "                context += (f\"  P{p['id']}: Pos=[L:{p['position'][0]:.1f}, F:{p['position'][1]:.0f}] \"\n",
    "                           f\"Vel=[{p['velocity'][0]:+.2f}, {p['velocity'][1]:+.2f}] \"\n",
    "                           f\"Acc:{p['fitness']:.4f}\\n\")\n",
    "        return context\n",
    "    \n",
    "    def get_c1_c2_history_string(self):\n",
    "        if not self.c1_c2_history:\n",
    "            return \"No c1/c2 history.\"\n",
    "        context = \"\\n=== C1/C2 HISTORY ===\\n\"\n",
    "        for h in self.c1_c2_history[-5:]:\n",
    "            context += f\"  Iter {h['iter']}: c1={h['c1']:.2f}, c2={h['c2']:.2f}\\n\"\n",
    "        return context\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "def get_data_loaders():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(Config.IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_dir = os.path.join(Config.DATASET_PATH, 'TRAIN')\n",
    "    test_dir = os.path.join(Config.DATASET_PATH, 'TEST')\n",
    "\n",
    "    try:\n",
    "        train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "        test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "        \n",
    "        if Config.TRAIN_DATA_PERCENTAGE < 1.0:\n",
    "            indices = list(range(len(train_dataset)))\n",
    "            split = int(np.floor(Config.TRAIN_DATA_PERCENTAGE * len(train_dataset)))\n",
    "            subset_indices = random.sample(indices, split)\n",
    "            train_dataset = Subset(train_dataset, subset_indices)\n",
    "            print(f\"  [Info] Using {len(train_dataset)} images ({Config.TRAIN_DATA_PERCENTAGE*100}%) for training.\")\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "        return train_loader, test_loader\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==========================================\n",
    "# 4. DYNAMIC CNN MODEL\n",
    "# ==========================================\n",
    "class DynamicCNN(nn.Module):\n",
    "    def __init__(self, num_layers, num_filters):\n",
    "        super(DynamicCNN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_channels = 3\n",
    "        \n",
    "        for _ in range(int(num_layers)):\n",
    "            self.convs.append(nn.Conv2d(in_channels, int(num_filters), 3, 1, 1))\n",
    "            self.convs.append(nn.MaxPool2d(2, 2))\n",
    "            in_channels = int(num_filters)\n",
    "        \n",
    "        final_dim = Config.IMG_SIZE[0]\n",
    "        for _ in range(int(num_layers)):\n",
    "            final_dim = final_dim // 2\n",
    "        if final_dim < 1:\n",
    "            final_dim = 1\n",
    "            \n",
    "        self.flatten_size = in_channels * final_dim * final_dim\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
    "        self.fc2 = nn.Linear(64, Config.NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.convs:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        x = x.view(-1, self.flatten_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ==========================================\n",
    "# 5. LLM INTERFACE (Returns c1, c2 + suggestions)\n",
    "# ==========================================\n",
    "def get_llm_suggestions_with_params(memory_ctx, particle_ctx, c1c2_ctx, current_c1, current_c2):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in PSO Hyperparameter Optimization for CNN Image Classification.\n",
    "\n",
    "OBJECTIVE: Maximize Classification Accuracy (Organic vs Recyclable waste).\n",
    "\n",
    "SEARCH SPACE:\n",
    "- Layers: {Config.MIN_LAYERS} to {Config.MAX_LAYERS}\n",
    "- Filters: {Config.MIN_FILTERS} to {Config.MAX_FILTERS}\n",
    "\n",
    "CURRENT PSO PARAMETERS:\n",
    "- c1 (cognitive): {current_c1:.2f}\n",
    "- c2 (social): {current_c2:.2f}\n",
    "\n",
    "{memory_ctx}\n",
    "\n",
    "{particle_ctx}\n",
    "\n",
    "{c1c2_ctx}\n",
    "\n",
    "=== YOUR TASKS ===\n",
    "\n",
    "1. ANALYZE the particle velocities:\n",
    "   - If velocities are too high (particles overshooting), suggest LOWER c1/c2.\n",
    "   - If velocities are too low (stagnation), suggest HIGHER c1/c2.\n",
    "   - Typical range: c1, c2 ∈ [1.0, 3.0]\n",
    "\n",
    "2. SUGGEST 2 new configurations [layers, filters] based on the history.\n",
    "\n",
    "3. RECOMMEND new c1 and c2 values.\n",
    "\n",
    "OUTPUT FORMAT (JSON only):\n",
    "{{\n",
    "    \"analysis\": \"Brief reasoning about velocity patterns...\",\n",
    "    \"suggestions\": [[layers1, filters1], [layers2, filters2]],\n",
    "    \"recommended_c1\": float,\n",
    "    \"recommended_c2\": float\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"  [LLM] Consulting for suggestions + c1/c2 tuning...\")\n",
    "        resp = requests.post(\n",
    "            Config.AVALAI_BASE_URL,\n",
    "            headers={\"Authorization\": f\"Bearer {Config.AVALAI_API_KEY}\", \"Content-Type\": \"application/json\"},\n",
    "            json={\"model\": Config.LLM_MODEL, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.5},\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            content = resp.json()['choices'][0]['message']['content']\n",
    "            content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            data = json.loads(content)\n",
    "            \n",
    "            print(f\"  [LLM Analysis]: {data.get('analysis', 'N/A')}\")\n",
    "            \n",
    "            suggestions = data.get(\"suggestions\", [])\n",
    "            new_c1 = np.clip(float(data.get(\"recommended_c1\", current_c1)), 1.0, 3.0)\n",
    "            new_c2 = np.clip(float(data.get(\"recommended_c2\", current_c2)), 1.0, 3.0)\n",
    "            \n",
    "            return suggestions, new_c1, new_c2\n",
    "        else:\n",
    "            print(f\"  [LLM Error] Status: {resp.status_code}\")\n",
    "            return [], current_c1, current_c2\n",
    "    except Exception as e:\n",
    "        print(f\"  [LLM Failed]: {e}\")\n",
    "        return [], current_c1, current_c2\n",
    "\n",
    "# ==========================================\n",
    "# 6. PSO WITH LLM-CONTROLLED C1/C2\n",
    "# ==========================================\n",
    "class LLMControlledPSO:\n",
    "    def __init__(self, train_loader, test_loader):\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.memory = OptimizationMemory()\n",
    "        \n",
    "        self.positions = np.zeros((Config.POPULATION_SIZE, 2))\n",
    "        self.positions[:, 0] = np.random.uniform(Config.MIN_LAYERS, Config.MAX_LAYERS, Config.POPULATION_SIZE)\n",
    "        self.positions[:, 1] = np.random.uniform(Config.MIN_FILTERS, Config.MAX_FILTERS, Config.POPULATION_SIZE)\n",
    "        \n",
    "        self.velocities = np.zeros((Config.POPULATION_SIZE, 2))\n",
    "        self.pbest_pos = self.positions.copy()\n",
    "        self.pbest_val = np.zeros(Config.POPULATION_SIZE)\n",
    "        self.gbest_pos = np.zeros(2)\n",
    "        self.gbest_val = 0.0\n",
    "        \n",
    "        self.c1 = Config.C1_INIT\n",
    "        self.c2 = Config.C2_INIT\n",
    "\n",
    "    def evaluate(self, pos):\n",
    "        L = int(np.clip(pos[0], Config.MIN_LAYERS, Config.MAX_LAYERS))\n",
    "        F_num = int(np.clip(pos[1], Config.MIN_FILTERS, Config.MAX_FILTERS))\n",
    "        print(f\"    > Eval: L={L}, F={F_num}...\", end=\" \")\n",
    "        \n",
    "        try:\n",
    "            model = DynamicCNN(L, F_num).to(Config.DEVICE)\n",
    "            opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "            model.train()\n",
    "            for img, lbl in self.train_loader:\n",
    "                opt.zero_grad()\n",
    "                F.cross_entropy(model(img.to(Config.DEVICE)), lbl.to(Config.DEVICE)).backward()\n",
    "                opt.step()\n",
    "            \n",
    "            model.eval()\n",
    "            corr, tot = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for img, lbl in self.test_loader:\n",
    "                    out = model(img.to(Config.DEVICE))\n",
    "                    corr += (out.argmax(1) == lbl.to(Config.DEVICE)).sum().item()\n",
    "                    tot += lbl.size(0)\n",
    "            acc = corr / tot\n",
    "            print(f\"Acc: {acc:.4f}\")\n",
    "            self.memory.add_record([L, F_num], acc)\n",
    "            return acc\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def run(self):\n",
    "        print(\"\\n=== Task 1: LLM-Controlled PSO (c1/c2 + Particle History) ===\")\n",
    "        \n",
    "        fitness = np.array([self.evaluate(p) for p in self.positions])\n",
    "        self.pbest_val = fitness.copy()\n",
    "        best_idx = fitness.argmax()\n",
    "        self.gbest_val = fitness[best_idx]\n",
    "        self.gbest_pos = self.positions[best_idx].copy()\n",
    "        \n",
    "        for iteration in range(Config.MAX_ITERATIONS):\n",
    "            w = Config.W_MAX - ((Config.W_MAX - Config.W_MIN) * iteration / Config.MAX_ITERATIONS)\n",
    "            print(f\"\\nIter {iteration+1}/{Config.MAX_ITERATIONS} | Best Acc: {self.gbest_val:.4f} | c1={self.c1:.2f}, c2={self.c2:.2f}\")\n",
    "            \n",
    "            self.memory.add_particle_state(iteration, self.positions, self.velocities, fitness)\n",
    "            \n",
    "            # LLM intervention\n",
    "            if iteration in [2, 5]:\n",
    "                mem_ctx = self.memory.get_context_string()\n",
    "                particle_ctx = self.memory.get_particle_state_string()\n",
    "                c1c2_ctx = self.memory.get_c1_c2_history_string()\n",
    "                \n",
    "                suggestions, new_c1, new_c2 = get_llm_suggestions_with_params(\n",
    "                    mem_ctx, particle_ctx, c1c2_ctx, self.c1, self.c2\n",
    "                )\n",
    "                \n",
    "                if new_c1 != self.c1 or new_c2 != self.c2:\n",
    "                    print(f\"  [LLM] Updating: c1={self.c1:.2f}->{new_c1:.2f}, c2={self.c2:.2f}->{new_c2:.2f}\")\n",
    "                    self.c1, self.c2 = new_c1, new_c2\n",
    "                    self.memory.add_c1_c2_record(self.c1, self.c2, iteration)\n",
    "                \n",
    "                if suggestions:\n",
    "                    worst_indices = np.argsort(fitness)[:len(suggestions)]\n",
    "                    for idx, sug in zip(worst_indices, suggestions):\n",
    "                        print(f\"  [LLM] Replacing P{idx} -> {sug}\")\n",
    "                        self.positions[idx] = np.array(sug)\n",
    "                        self.velocities[idx] = np.zeros(2)\n",
    "                        new_fit = self.evaluate(self.positions[idx])\n",
    "                        fitness[idx] = new_fit\n",
    "                        if new_fit > self.gbest_val:\n",
    "                            self.gbest_val = new_fit\n",
    "                            self.gbest_pos = self.positions[idx].copy()\n",
    "                            print(f\"  *** LLM FOUND NEW GLOBAL BEST! ***\")\n",
    "            \n",
    "            # PSO update\n",
    "            for i in range(Config.POPULATION_SIZE):\n",
    "                r1, r2 = np.random.rand(2), np.random.rand(2)\n",
    "                social_term = 0 if i == Config.POPULATION_SIZE - 1 else self.c2 * r2 * (self.gbest_pos - self.positions[i])\n",
    "                cognitive_term = self.c1 * r1 * (self.pbest_pos[i] - self.positions[i])\n",
    "                \n",
    "                self.velocities[i] = w * self.velocities[i] + cognitive_term + social_term\n",
    "                self.positions[i] += self.velocities[i]\n",
    "                self.positions[i, 0] = np.clip(self.positions[i, 0], Config.MIN_LAYERS, Config.MAX_LAYERS)\n",
    "                self.positions[i, 1] = np.clip(self.positions[i, 1], Config.MIN_FILTERS, Config.MAX_FILTERS)\n",
    "                \n",
    "                acc = self.evaluate(self.positions[i])\n",
    "                fitness[i] = acc\n",
    "                \n",
    "                if acc > self.pbest_val[i]:\n",
    "                    self.pbest_val[i] = acc\n",
    "                    self.pbest_pos[i] = self.positions[i].copy()\n",
    "                if acc > self.gbest_val:\n",
    "                    self.gbest_val = acc\n",
    "                    self.gbest_pos = self.positions[i].copy()\n",
    "        \n",
    "        return self.gbest_pos, self.gbest_val\n",
    "\n",
    "# ==========================================\n",
    "# 7. MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CLASSIFICATION TASK 1: LLM Controls c1/c2 + Particle History\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    train_loader, test_loader = get_data_loaders()\n",
    "    if train_loader is None:\n",
    "        return\n",
    "    \n",
    "    start = time.time()\n",
    "    pso = LLMControlledPSO(train_loader, test_loader)\n",
    "    best_pos, best_acc = pso.run()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"RESULT: Best Params = [L={int(best_pos[0])}, F={int(best_pos[1])}]\")\n",
    "    print(f\"        Best Accuracy = {best_acc*100:.2f}%\")\n",
    "    print(f\"        Time = {elapsed:.1f}s\")\n",
    "    print(f\"        Final c1={pso.c1:.2f}, c2={pso.c2:.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T05:16:35.771357Z",
     "iopub.status.busy": "2025-12-15T05:16:35.771128Z",
     "iopub.status.idle": "2025-12-15T05:58:38.662779Z",
     "shell.execute_reply": "2025-12-15T05:58:38.662048Z",
     "shell.execute_reply.started": "2025-12-15T05:16:35.771339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "CLASSIFICATION TASK 2: Expanded 5D Search Space\n",
      "==========================================================================================\n",
      "  [Info] Using 5641 images for training.\n",
      "\n",
      "--------------------------------------------------\n",
      "Running VANILLA 5D PSO...\n",
      "\n",
      "=== Starting 5D Vanilla PSO ===\n",
      "    > Eval: L=4, F=70, LR=0.0059, Drop=0.13, Ep=1... Acc: 0.8615\n",
      "    > Eval: L=4, F=81, LR=0.0098, Drop=0.37, Ep=1... Acc: 0.5575\n",
      "    > Eval: L=2, F=55, LR=0.0004, Drop=0.43, Ep=2... Acc: 0.8715\n",
      "    > Eval: L=3, F=81, LR=0.0049, Drop=0.19, Ep=3... Acc: 0.7680\n",
      "    > Eval: L=2, F=121, LR=0.0046, Drop=0.01, Ep=1... Acc: 0.8026\n",
      "\n",
      "Iter 1/8 | Best Acc: 0.8715\n",
      "    > Eval: L=3, F=69, LR=0.0001, Drop=0.14, Ep=2... Acc: 0.8830\n",
      "    > Eval: L=3, F=74, LR=0.0001, Drop=0.23, Ep=2... Acc: 0.8794\n",
      "    > Eval: L=3, F=77, LR=0.0001, Drop=0.10, Ep=2... Acc: 0.8834\n",
      "    > Eval: L=3, F=79, LR=0.0001, Drop=0.19, Ep=2... Acc: 0.8743\n",
      "    > Eval: L=2, F=121, LR=0.0046, Drop=0.01, Ep=1... Acc: 0.8703\n",
      "\n",
      "Iter 2/8 | Best Acc: 0.8834\n",
      "    > Eval: L=3, F=70, LR=0.0001, Drop=0.10, Ep=3... Acc: 0.8735\n",
      "    > Eval: L=2, F=73, LR=0.0001, Drop=0.00, Ep=2... Acc: 0.8782\n",
      "    > Eval: L=3, F=96, LR=0.0001, Drop=0.00, Ep=2... Acc: 0.8679\n",
      "    > Eval: L=3, F=75, LR=0.0001, Drop=0.06, Ep=2... Acc: 0.8603\n",
      "    > Eval: L=2, F=121, LR=0.0046, Drop=0.01, Ep=1... Acc: 0.8667\n",
      "\n",
      "Iter 3/8 | Best Acc: 0.8834\n",
      "    > Eval: L=3, F=75, LR=0.0001, Drop=0.12, Ep=2... Acc: 0.8842\n",
      "    > Eval: L=3, F=74, LR=0.0001, Drop=0.11, Ep=1... Acc: 0.8627\n",
      "    > Eval: L=2, F=70, LR=0.0001, Drop=0.05, Ep=2... Acc: 0.8528\n",
      "    > Eval: L=3, F=78, LR=0.0001, Drop=0.09, Ep=1... Acc: 0.8691\n",
      "    > Eval: L=2, F=121, LR=0.0046, Drop=0.01, Ep=1... Acc: 0.5575\n",
      "\n",
      "Iter 4/8 | Best Acc: 0.8842\n",
      "    > Eval: L=2, F=78, LR=0.0001, Drop=0.13, Ep=1... Acc: 0.8400\n",
      "    > Eval: L=4, F=75, LR=0.0001, Drop=0.28, Ep=1... Acc: 0.8631\n",
      "    > Eval: L=3, F=67, LR=0.0001, Drop=0.23, Ep=1... Acc: 0.8587\n",
      "    > Eval: L=2, F=80, LR=0.0001, Drop=0.22, Ep=2... Acc: 0.8731\n",
      "    > Eval: L=2, F=121, LR=0.0046, Drop=0.01, Ep=1... Acc: 0.7000\n",
      "\n",
      "Iter 5/8 | Best Acc: 0.8842\n",
      "    > Eval: L=3, F=74, LR=0.0001, Drop=0.10, Ep=3... Acc: 0.8778\n",
      "    > Eval: L=2, F=74, LR=0.0001, Drop=0.28, Ep=2... Acc: 0.8524\n",
      "    > Eval: L=3, F=74, LR=0.0001, Drop=0.15, Ep=2... Acc: 0.8699\n",
      "    > Eval: L=3, F=79, LR=0.0001, Drop=0.26, Ep=2... Acc: 0.8719\n",
      "    > Eval: L=2, F=121, LR=0.0046, Drop=0.01, Ep=1... Acc: 0.7402\n",
      "\n",
      "Iter 6/8 | Best Acc: 0.8842\n",
      "    > Eval: L=3, F=73, LR=0.0001, Drop=0.14, Ep=3... Acc: 0.8480\n",
      "    > Eval: L=3, F=73, LR=0.0001, Drop=0.17, Ep=1... Acc: 0.8416\n",
      "    > Eval: L=2, F=81, LR=0.0001, Drop=0.05, Ep=1... Acc: 0.8723\n",
      "    > Eval: L=3, F=74, LR=0.0001, Drop=0.00, Ep=2... Acc: 0.8750\n",
      "    > Eval: L=2, F=121, LR=0.0046, Drop=0.01, Ep=1... Acc: 0.5575\n",
      "\n",
      "Iter 7/8 | Best Acc: 0.8842\n",
      "    > Eval: L=3, F=74, LR=0.0001, Drop=0.11, Ep=1... Acc: 0.8739\n",
      "    > Eval: L=4, F=76, LR=0.0001, Drop=0.12, Ep=1... Acc: 0.8448\n",
      "    > Eval: L=3, F=68, LR=0.0001, Drop=0.14, Ep=1... Acc: 0.8647\n",
      "    > Eval: L=2, F=71, LR=0.0001, Drop=0.00, Ep=2... Acc: 0.8699\n",
      "    > Eval: L=2, F=121, LR=0.0046, Drop=0.01, Ep=1... Acc: 0.8205\n",
      "\n",
      "Iter 8/8 | Best Acc: 0.8842\n",
      "    > Eval: L=2, F=76, LR=0.0001, Drop=0.10, Ep=1... Acc: 0.8739\n",
      "    > Eval: L=3, F=75, LR=0.0001, Drop=0.11, Ep=2... Acc: 0.8735\n",
      "    > Eval: L=3, F=83, LR=0.0001, Drop=0.13, Ep=1... Acc: 0.8727\n",
      "    > Eval: L=3, F=78, LR=0.0001, Drop=0.18, Ep=2... Acc: 0.8735\n",
      "    > Eval: L=2, F=121, LR=0.0046, Drop=0.01, Ep=1... Acc: 0.5575\n",
      "\n",
      "--------------------------------------------------\n",
      "Running LLM-ENHANCED 5D PSO...\n",
      "\n",
      "=== Starting 5D LLM-Enhanced PSO ===\n",
      "    > Eval: L=2, F=99, LR=0.0072, Drop=0.12, Ep=3... Acc: 0.8627\n",
      "    > Eval: L=4, F=81, LR=0.0019, Drop=0.24, Ep=2... Acc: 0.8456\n",
      "    > Eval: L=2, F=32, LR=0.0021, Drop=0.49, Ep=2... Acc: 0.8711\n",
      "    > Eval: L=2, F=22, LR=0.0035, Drop=0.28, Ep=2... Acc: 0.8142\n",
      "    > Eval: L=3, F=77, LR=0.0038, Drop=0.20, Ep=2... Acc: 0.8603\n",
      "\n",
      "Iter 1/8 | Best Acc: 0.8711\n",
      "    > Eval: L=2, F=16, LR=0.0034, Drop=0.50, Ep=1... Acc: 0.8368\n",
      "    > Eval: L=3, F=22, LR=0.0020, Drop=0.25, Ep=2... Acc: 0.8587\n",
      "    > Eval: L=2, F=32, LR=0.0021, Drop=0.49, Ep=2... Acc: 0.8544\n",
      "    > Eval: L=2, F=29, LR=0.0034, Drop=0.50, Ep=1... Acc: 0.8552\n",
      "    > Eval: L=3, F=77, LR=0.0038, Drop=0.20, Ep=2... Acc: 0.8655\n",
      "\n",
      "Iter 2/8 | Best Acc: 0.8711\n",
      "    > Eval: L=2, F=16, LR=0.0007, Drop=0.50, Ep=2... Acc: 0.8392\n",
      "    > Eval: L=2, F=16, LR=0.0022, Drop=0.29, Ep=2... Acc: 0.8639\n",
      "    > Eval: L=2, F=32, LR=0.0021, Drop=0.49, Ep=2... Acc: 0.8699\n",
      "    > Eval: L=2, F=37, LR=0.0017, Drop=0.50, Ep=1... Acc: 0.8552\n",
      "    > Eval: L=3, F=77, LR=0.0038, Drop=0.20, Ep=2... Acc: 0.8436\n",
      "\n",
      "Iter 3/8 | Best Acc: 0.8711\n",
      "  [LLM] Consulting for 5D suggestions...\n",
      "  [LLM Analysis]: Best configs cluster around 2–3 layers, moderate filters (32–100), LR ≈ 0.002–0.004, dropout ≈ 0.12–0.49, epochs=2–3. Worst configs show that too low LR (0.0007) and max dropout (0.5) hurt performance, and 1 epoch underperforms. I propose one exploitation config close to the best-known region with slight exploration in filters and reduced dropout, and one exploratory config with 3 layers and slightly higher filters but still within the successful LR/epoch range.\n",
      "  [LLM] Replacing P0 -> [2, 40, 0.0023, 0.35, 2]\n",
      "    > Eval: L=2, F=40, LR=0.0023, Drop=0.35, Ep=2... Acc: 0.8540\n",
      "  [LLM] Replacing P4 -> [3, 64, 0.0032, 0.22, 2]\n",
      "    > Eval: L=3, F=64, LR=0.0032, Drop=0.22, Ep=2... Acc: 0.8663\n",
      "    > Eval: L=2, F=87, LR=0.0100, Drop=0.39, Ep=2... Acc: 0.8357\n",
      "    > Eval: L=1, F=16, LR=0.0021, Drop=0.45, Ep=2... Acc: 0.8758\n",
      "    > Eval: L=1, F=16, LR=0.0021, Drop=0.46, Ep=2... Acc: 0.8404\n",
      "    > Eval: L=1, F=16, LR=0.0017, Drop=0.50, Ep=3... Acc: 0.8317\n",
      "    > Eval: L=3, F=84, LR=0.0032, Drop=0.20, Ep=2... Acc: 0.5575\n",
      "\n",
      "Iter 4/8 | Best Acc: 0.8758\n",
      "    > Eval: L=1, F=109, LR=0.0100, Drop=0.00, Ep=3... Acc: 0.8583\n",
      "    > Eval: L=1, F=16, LR=0.0020, Drop=0.50, Ep=2... Acc: 0.8484\n",
      "    > Eval: L=1, F=16, LR=0.0021, Drop=0.44, Ep=2... Acc: 0.8735\n",
      "    > Eval: L=2, F=16, LR=0.0034, Drop=0.50, Ep=3... Acc: 0.8679\n",
      "    > Eval: L=3, F=95, LR=0.0041, Drop=0.19, Ep=2... Acc: 0.8627\n",
      "\n",
      "Iter 5/8 | Best Acc: 0.8758\n",
      "    > Eval: L=2, F=16, LR=0.0001, Drop=0.32, Ep=1... Acc: 0.8571\n",
      "    > Eval: L=1, F=16, LR=0.0022, Drop=0.44, Ep=2... Acc: 0.8189\n",
      "    > Eval: L=1, F=16, LR=0.0021, Drop=0.44, Ep=2... Acc: 0.8408\n",
      "    > Eval: L=2, F=16, LR=0.0027, Drop=0.47, Ep=2... Acc: 0.8126\n",
      "    > Eval: L=3, F=87, LR=0.0041, Drop=0.20, Ep=2... Acc: 0.8333\n",
      "\n",
      "Iter 6/8 | Best Acc: 0.8758\n",
      "  [LLM] Consulting for 5D suggestions...\n",
      "  [LLM Analysis]: Best configs cluster around: few layers (1–2), small filters (16–32), LR ≈ 0.002–0.0035, high dropout (0.44–0.50), and 2–3 epochs. Worst configs suggest that mid dropout (~0.28) and higher filters with more layers hurt performance. So we should (1) locally refine near the current best (L=1, F=16, LR≈0.0021, high dropout, 2–3 epochs) and (2) test a slightly deeper but still small model (L=2, F=16–32) with tuned LR and high dropout. I keep LR close to 0.002–0.003, avoid low dropout, and avoid large filters or many layers.\n",
      "  [LLM] Replacing P3 -> [1, 16, 0.0023, 0.46, 3]\n",
      "    > Eval: L=1, F=16, LR=0.0023, Drop=0.46, Ep=3... Acc: 0.8758\n",
      "  [LLM] Replacing P1 -> [2, 16, 0.0028, 0.48, 2]\n",
      "    > Eval: L=2, F=16, LR=0.0028, Drop=0.48, Ep=2... Acc: 0.8130\n",
      "    > Eval: L=1, F=16, LR=0.0062, Drop=0.47, Ep=1... Acc: 0.8687\n",
      "    > Eval: L=1, F=16, LR=0.0012, Drop=0.40, Ep=2... Acc: 0.8631\n",
      "    > Eval: L=1, F=16, LR=0.0021, Drop=0.46, Ep=2... Acc: 0.8476\n",
      "    > Eval: L=2, F=16, LR=0.0037, Drop=0.48, Ep=1... Acc: 0.8392\n",
      "    > Eval: L=3, F=66, LR=0.0039, Drop=0.20, Ep=2... Acc: 0.8587\n",
      "\n",
      "Iter 7/8 | Best Acc: 0.8758\n",
      "    > Eval: L=1, F=16, LR=0.0054, Drop=0.50, Ep=1... Acc: 0.8603\n",
      "    > Eval: L=1, F=16, LR=0.0023, Drop=0.38, Ep=2... Acc: 0.8663\n",
      "    > Eval: L=1, F=16, LR=0.0021, Drop=0.44, Ep=2... Acc: 0.8627\n",
      "    > Eval: L=2, F=16, LR=0.0030, Drop=0.45, Ep=1... Acc: 0.8687\n",
      "    > Eval: L=3, F=74, LR=0.0037, Drop=0.21, Ep=2... Acc: 0.8667\n",
      "\n",
      "Iter 8/8 | Best Acc: 0.8758\n",
      "    > Eval: L=1, F=16, LR=0.0033, Drop=0.44, Ep=1... Acc: 0.8524\n",
      "    > Eval: L=1, F=16, LR=0.0022, Drop=0.50, Ep=2... Acc: 0.8735\n",
      "    > Eval: L=1, F=16, LR=0.0021, Drop=0.44, Ep=2... Acc: 0.8556\n",
      "    > Eval: L=2, F=16, LR=0.0008, Drop=0.44, Ep=2... Acc: 0.8671\n",
      "    > Eval: L=3, F=78, LR=0.0036, Drop=0.21, Ep=2... Acc: 0.8639\n",
      "\n",
      "==========================================================================================\n",
      "METHOD               | L   | F    | LR       | DROP  | EP  | ACC      | TIME\n",
      "------------------------------------------------------------------------------------------\n",
      "Vanilla 5D PSO       | 3   | 75   | 0.0001   | 0.12  | 2   | 88.42  % | 1127.2s\n",
      "LLM-Enhanced 5D      | 1   | 16   | 0.0021   | 0.45  | 2   | 87.58  % | 1385.9s\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Classification_Task2_5D_SearchSpace.py\n",
    "# Expanded search space: Layers, Filters, Learning Rate, Dropout, Epochs\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (5D SEARCH SPACE)\n",
    "# ==========================================\n",
    "class Config:\n",
    "    AVALAI_API_KEY = \"\"\n",
    "    AVALAI_BASE_URL = \"https://api.avalai.ir/v1/chat/completions\"\n",
    "    LLM_MODEL = \"gpt-5.1\"\n",
    "\n",
    "    DATASET_PATH = \"/kaggle/input/waste-classification-data/DATASET\"\n",
    "    IMG_SIZE = (128, 128)\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_CLASSES = 2\n",
    "    TRAIN_DATA_PERCENTAGE = 0.25\n",
    "\n",
    "    POPULATION_SIZE = 5\n",
    "    MAX_ITERATIONS = 8\n",
    "    C1 = 2.0\n",
    "    C2 = 2.0\n",
    "    W_MAX = 0.9\n",
    "    W_MIN = 0.4\n",
    "\n",
    "    # EXPANDED 5D SEARCH SPACE\n",
    "    BOUNDS = {\n",
    "        'layers_min': 1, 'layers_max': 4,\n",
    "        'filters_min': 16, 'filters_max': 128,\n",
    "        'lr_min': 0.0001, 'lr_max': 0.01,\n",
    "        'dropout_min': 0.0, 'dropout_max': 0.5,\n",
    "        'epochs_min': 1, 'epochs_max': 3\n",
    "    }\n",
    "    \n",
    "    DIM_LAYERS = 0\n",
    "    DIM_FILTERS = 1\n",
    "    DIM_LR = 2\n",
    "    DIM_DROPOUT = 3\n",
    "    DIM_EPOCHS = 4\n",
    "    NUM_DIMS = 5\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. MEMORY MODULE\n",
    "# ==========================================\n",
    "class OptimizationMemory:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "    \n",
    "    def add_record(self, params, acc):\n",
    "        self.history.append({'params': params, 'acc': float(acc)})\n",
    "    \n",
    "    def get_context_string(self):\n",
    "        if not self.history:\n",
    "            return \"No history yet.\"\n",
    "        sorted_hist = sorted(self.history, key=lambda x: x['acc'], reverse=True)\n",
    "        best_5 = sorted_hist[:5]\n",
    "        worst_3 = sorted_hist[-3:]\n",
    "        \n",
    "        context = \"=== OPTIMIZATION MEMORY (5D) ===\\n\"\n",
    "        context += \"TOP 5 Configurations:\\n\"\n",
    "        for i, h in enumerate(best_5):\n",
    "            p = h['params']\n",
    "            context += f\"  {i+1}. [L={p[0]}, F={p[1]}, LR={p[2]:.4f}, Drop={p[3]:.2f}, Ep={p[4]}] -> Acc: {h['acc']:.4f}\\n\"\n",
    "        context += \"\\nWORST 3:\\n\"\n",
    "        for i, h in enumerate(worst_3):\n",
    "            p = h['params']\n",
    "            context += f\"  {i+1}. [L={p[0]}, F={p[1]}, LR={p[2]:.4f}, Drop={p[3]:.2f}, Ep={p[4]}] -> Acc: {h['acc']:.4f}\\n\"\n",
    "        return context\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "def get_data_loaders():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(Config.IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_dir = os.path.join(Config.DATASET_PATH, 'TRAIN')\n",
    "    test_dir = os.path.join(Config.DATASET_PATH, 'TEST')\n",
    "\n",
    "    try:\n",
    "        train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "        test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "        \n",
    "        if Config.TRAIN_DATA_PERCENTAGE < 1.0:\n",
    "            indices = list(range(len(train_dataset)))\n",
    "            split = int(np.floor(Config.TRAIN_DATA_PERCENTAGE * len(train_dataset)))\n",
    "            subset_indices = random.sample(indices, split)\n",
    "            train_dataset = Subset(train_dataset, subset_indices)\n",
    "            print(f\"  [Info] Using {len(train_dataset)} images for training.\")\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "        return train_loader, test_loader\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==========================================\n",
    "# 4. DYNAMIC CNN MODEL (with Dropout)\n",
    "# ==========================================\n",
    "class DynamicCNN(nn.Module):\n",
    "    def __init__(self, num_layers, num_filters, dropout_rate):\n",
    "        super(DynamicCNN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_channels = 3\n",
    "        \n",
    "        for _ in range(int(num_layers)):\n",
    "            self.convs.append(nn.Conv2d(in_channels, int(num_filters), 3, 1, 1))\n",
    "            self.convs.append(nn.MaxPool2d(2, 2))\n",
    "            in_channels = int(num_filters)\n",
    "        \n",
    "        final_dim = Config.IMG_SIZE[0]\n",
    "        for _ in range(int(num_layers)):\n",
    "            final_dim = final_dim // 2\n",
    "        if final_dim < 1:\n",
    "            final_dim = 1\n",
    "            \n",
    "        self.flatten_size = in_channels * final_dim * final_dim\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
    "        self.fc2 = nn.Linear(64, Config.NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.convs:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        x = x.view(-1, self.flatten_size)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ==========================================\n",
    "# 5. LLM INTERFACE (5D)\n",
    "# ==========================================\n",
    "def get_llm_suggestions_5d(memory_ctx):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in PSO Hyperparameter Optimization for CNN Image Classification.\n",
    "\n",
    "OBJECTIVE: Maximize Accuracy (Organic vs Recyclable waste).\n",
    "\n",
    "SEARCH SPACE (5 Dimensions):\n",
    "1. Layers: {Config.BOUNDS['layers_min']} to {Config.BOUNDS['layers_max']}\n",
    "2. Filters: {Config.BOUNDS['filters_min']} to {Config.BOUNDS['filters_max']}\n",
    "3. Learning Rate: {Config.BOUNDS['lr_min']} to {Config.BOUNDS['lr_max']}\n",
    "4. Dropout: {Config.BOUNDS['dropout_min']} to {Config.BOUNDS['dropout_max']}\n",
    "5. Epochs: {Config.BOUNDS['epochs_min']} to {Config.BOUNDS['epochs_max']}\n",
    "\n",
    "{memory_ctx}\n",
    "\n",
    "TASK: Suggest 2 new configurations based on patterns.\n",
    "\n",
    "OUTPUT FORMAT (JSON only):\n",
    "{{\n",
    "    \"analysis\": \"Brief reasoning...\",\n",
    "    \"suggestions\": [\n",
    "        [layers1, filters1, lr1, dropout1, epochs1],\n",
    "        [layers2, filters2, lr2, dropout2, epochs2]\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"  [LLM] Consulting for 5D suggestions...\")\n",
    "        resp = requests.post(\n",
    "            Config.AVALAI_BASE_URL,\n",
    "            headers={\"Authorization\": f\"Bearer {Config.AVALAI_API_KEY}\", \"Content-Type\": \"application/json\"},\n",
    "            json={\"model\": Config.LLM_MODEL, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.6},\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            content = resp.json()['choices'][0]['message']['content']\n",
    "            content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            data = json.loads(content)\n",
    "            print(f\"  [LLM Analysis]: {data.get('analysis', 'N/A')}\")\n",
    "            return data.get(\"suggestions\", [])\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"  [LLM Failed]: {e}\")\n",
    "        return []\n",
    "\n",
    "# ==========================================\n",
    "# 6. 5D PSO\n",
    "# ==========================================\n",
    "class Expanded5DPSO:\n",
    "    def __init__(self, train_loader, test_loader, use_llm=True):\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.use_llm = use_llm\n",
    "        self.memory = OptimizationMemory()\n",
    "        \n",
    "        self.positions = np.zeros((Config.POPULATION_SIZE, Config.NUM_DIMS))\n",
    "        self.positions[:, Config.DIM_LAYERS] = np.random.uniform(Config.BOUNDS['layers_min'], Config.BOUNDS['layers_max'], Config.POPULATION_SIZE)\n",
    "        self.positions[:, Config.DIM_FILTERS] = np.random.uniform(Config.BOUNDS['filters_min'], Config.BOUNDS['filters_max'], Config.POPULATION_SIZE)\n",
    "        self.positions[:, Config.DIM_LR] = np.random.uniform(Config.BOUNDS['lr_min'], Config.BOUNDS['lr_max'], Config.POPULATION_SIZE)\n",
    "        self.positions[:, Config.DIM_DROPOUT] = np.random.uniform(Config.BOUNDS['dropout_min'], Config.BOUNDS['dropout_max'], Config.POPULATION_SIZE)\n",
    "        self.positions[:, Config.DIM_EPOCHS] = np.random.uniform(Config.BOUNDS['epochs_min'], Config.BOUNDS['epochs_max'], Config.POPULATION_SIZE)\n",
    "        \n",
    "        self.velocities = np.zeros((Config.POPULATION_SIZE, Config.NUM_DIMS))\n",
    "        self.pbest_pos = self.positions.copy()\n",
    "        self.pbest_val = np.zeros(Config.POPULATION_SIZE)\n",
    "        self.gbest_pos = np.zeros(Config.NUM_DIMS)\n",
    "        self.gbest_val = 0.0\n",
    "\n",
    "    def clip_position(self, pos):\n",
    "        pos[Config.DIM_LAYERS] = np.clip(pos[Config.DIM_LAYERS], Config.BOUNDS['layers_min'], Config.BOUNDS['layers_max'])\n",
    "        pos[Config.DIM_FILTERS] = np.clip(pos[Config.DIM_FILTERS], Config.BOUNDS['filters_min'], Config.BOUNDS['filters_max'])\n",
    "        pos[Config.DIM_LR] = np.clip(pos[Config.DIM_LR], Config.BOUNDS['lr_min'], Config.BOUNDS['lr_max'])\n",
    "        pos[Config.DIM_DROPOUT] = np.clip(pos[Config.DIM_DROPOUT], Config.BOUNDS['dropout_min'], Config.BOUNDS['dropout_max'])\n",
    "        pos[Config.DIM_EPOCHS] = np.clip(pos[Config.DIM_EPOCHS], Config.BOUNDS['epochs_min'], Config.BOUNDS['epochs_max'])\n",
    "        return pos\n",
    "\n",
    "    def decode_position(self, pos):\n",
    "        return {\n",
    "            'layers': int(np.round(pos[Config.DIM_LAYERS])),\n",
    "            'filters': int(np.round(pos[Config.DIM_FILTERS])),\n",
    "            'lr': float(pos[Config.DIM_LR]),\n",
    "            'dropout': float(pos[Config.DIM_DROPOUT]),\n",
    "            'epochs': int(np.round(pos[Config.DIM_EPOCHS]))\n",
    "        }\n",
    "\n",
    "    def evaluate(self, position):\n",
    "        params = self.decode_position(position)\n",
    "        print(f\"    > Eval: L={params['layers']}, F={params['filters']}, LR={params['lr']:.4f}, Drop={params['dropout']:.2f}, Ep={params['epochs']}...\", end=\" \")\n",
    "        \n",
    "        try:\n",
    "            model = DynamicCNN(params['layers'], params['filters'], params['dropout']).to(Config.DEVICE)\n",
    "            opt = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "            \n",
    "            model.train()\n",
    "            for _ in range(params['epochs']):\n",
    "                for img, lbl in self.train_loader:\n",
    "                    opt.zero_grad()\n",
    "                    F.cross_entropy(model(img.to(Config.DEVICE)), lbl.to(Config.DEVICE)).backward()\n",
    "                    opt.step()\n",
    "            \n",
    "            model.eval()\n",
    "            corr, tot = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for img, lbl in self.test_loader:\n",
    "                    out = model(img.to(Config.DEVICE))\n",
    "                    corr += (out.argmax(1) == lbl.to(Config.DEVICE)).sum().item()\n",
    "                    tot += lbl.size(0)\n",
    "            acc = corr / tot\n",
    "            print(f\"Acc: {acc:.4f}\")\n",
    "            self.memory.add_record([params['layers'], params['filters'], params['lr'], params['dropout'], params['epochs']], acc)\n",
    "            return acc\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def run(self):\n",
    "        mode = \"LLM-Enhanced\" if self.use_llm else \"Vanilla\"\n",
    "        print(f\"\\n=== Starting 5D {mode} PSO ===\")\n",
    "        \n",
    "        fitness = np.array([self.evaluate(p) for p in self.positions])\n",
    "        self.pbest_val = fitness.copy()\n",
    "        best_idx = fitness.argmax()\n",
    "        self.gbest_val = fitness[best_idx]\n",
    "        self.gbest_pos = self.positions[best_idx].copy()\n",
    "        \n",
    "        for iteration in range(Config.MAX_ITERATIONS):\n",
    "            w = Config.W_MAX - ((Config.W_MAX - Config.W_MIN) * iteration / Config.MAX_ITERATIONS)\n",
    "            print(f\"\\nIter {iteration+1}/{Config.MAX_ITERATIONS} | Best Acc: {self.gbest_val:.4f}\")\n",
    "            \n",
    "            if self.use_llm and iteration in [2, 5]:\n",
    "                suggestions = get_llm_suggestions_5d(self.memory.get_context_string())\n",
    "                if suggestions:\n",
    "                    worst_indices = np.argsort(fitness)[:len(suggestions)]\n",
    "                    for idx, sug in zip(worst_indices, suggestions):\n",
    "                        print(f\"  [LLM] Replacing P{idx} -> {sug}\")\n",
    "                        self.positions[idx] = np.array(sug)\n",
    "                        self.velocities[idx] = np.zeros(Config.NUM_DIMS)\n",
    "                        new_fit = self.evaluate(self.positions[idx])\n",
    "                        fitness[idx] = new_fit\n",
    "                        if new_fit > self.gbest_val:\n",
    "                            self.gbest_val = new_fit\n",
    "                            self.gbest_pos = self.positions[idx].copy()\n",
    "                            print(f\"  *** NEW GLOBAL BEST! ***\")\n",
    "            \n",
    "            for i in range(Config.POPULATION_SIZE):\n",
    "                r1 = np.random.rand(Config.NUM_DIMS)\n",
    "                r2 = np.random.rand(Config.NUM_DIMS)\n",
    "                \n",
    "                social_term = 0 if i == Config.POPULATION_SIZE - 1 else Config.C2 * r2 * (self.gbest_pos - self.positions[i])\n",
    "                cognitive_term = Config.C1 * r1 * (self.pbest_pos[i] - self.positions[i])\n",
    "                \n",
    "                self.velocities[i] = w * self.velocities[i] + cognitive_term + social_term\n",
    "                self.positions[i] += self.velocities[i]\n",
    "                self.positions[i] = self.clip_position(self.positions[i])\n",
    "                \n",
    "                acc = self.evaluate(self.positions[i])\n",
    "                fitness[i] = acc\n",
    "                \n",
    "                if acc > self.pbest_val[i]:\n",
    "                    self.pbest_val[i] = acc\n",
    "                    self.pbest_pos[i] = self.positions[i].copy()\n",
    "                if acc > self.gbest_val:\n",
    "                    self.gbest_val = acc\n",
    "                    self.gbest_pos = self.positions[i].copy()\n",
    "        \n",
    "        return self.gbest_pos, self.gbest_val\n",
    "\n",
    "# ==========================================\n",
    "# 7. MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"CLASSIFICATION TASK 2: Expanded 5D Search Space\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    train_loader, test_loader = get_data_loaders()\n",
    "    if train_loader is None:\n",
    "        return\n",
    "    \n",
    "    # Vanilla 5D PSO\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"Running VANILLA 5D PSO...\")\n",
    "    start = time.time()\n",
    "    pso_v = Expanded5DPSO(train_loader, test_loader, use_llm=False)\n",
    "    best_v, acc_v = pso_v.run()\n",
    "    time_v = time.time() - start\n",
    "    \n",
    "    # LLM-Enhanced 5D PSO\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"Running LLM-ENHANCED 5D PSO...\")\n",
    "    start = time.time()\n",
    "    pso_l = Expanded5DPSO(train_loader, test_loader, use_llm=True)\n",
    "    best_l, acc_l = pso_l.run()\n",
    "    time_l = time.time() - start\n",
    "    \n",
    "    # Results\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"{'METHOD':<20} | {'L':<3} | {'F':<4} | {'LR':<8} | {'DROP':<5} | {'EP':<3} | {'ACC':<8} | {'TIME'}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    v_p = pso_v.decode_position(best_v)\n",
    "    l_p = pso_l.decode_position(best_l)\n",
    "    \n",
    "    print(f\"{'Vanilla 5D PSO':<20} | {v_p['layers']:<3} | {v_p['filters']:<4} | {v_p['lr']:<8.4f} | {v_p['dropout']:<5.2f} | {v_p['epochs']:<3} | {acc_v*100:<7.2f}% | {time_v:.1f}s\")\n",
    "    print(f\"{'LLM-Enhanced 5D':<20} | {l_p['layers']:<3} | {l_p['filters']:<4} | {l_p['lr']:<8.4f} | {l_p['dropout']:<5.2f} | {l_p['epochs']:<3} | {acc_l*100:<7.2f}% | {time_l:.1f}s\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T06:30:00.193244Z",
     "iopub.status.busy": "2025-12-16T06:30:00.192500Z",
     "iopub.status.idle": "2025-12-16T07:48:03.550478Z",
     "shell.execute_reply": "2025-12-16T07:48:03.549253Z",
     "shell.execute_reply.started": "2025-12-16T06:30:00.193206Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "CLASSIFICATION TASK 3: Compare Inertia Weight Strategies\n",
      "Based on: 'A novel PSO algorithm with adaptive inertia weight'\n",
      "====================================================================================================\n",
      "  [Info] Using 5641 images for training.\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== PSO with W Strategy: LINEAR_DECREASE ===\n",
      "    > Eval: L=2, F=58... Acc: 0.8524\n",
      "    > Eval: L=3, F=29... Acc: 0.8766\n",
      "    > Eval: L=3, F=103... Acc: 0.8548\n",
      "    > Eval: L=3, F=28... Acc: 0.8508\n",
      "    > Eval: L=3, F=24... Acc: 0.8424\n",
      "\n",
      "Iter 1/8 | Best Acc: 0.8766 | W=0.900 | Success Rate=1.00\n",
      "    > Eval: L=4, F=16... Acc: 0.8508\n",
      "    > Eval: L=3, F=29... Acc: 0.8595\n",
      "    > Eval: L=3, F=40... Acc: 0.8587\n",
      "    > Eval: L=3, F=29... Acc: 0.8647\n",
      "    > Eval: L=3, F=24... Acc: 0.8540\n",
      "\n",
      "Iter 2/8 | Best Acc: 0.8766 | W=0.838 | Success Rate=0.00\n",
      "    > Eval: L=2, F=22... Acc: 0.8687\n",
      "    > Eval: L=3, F=29... Acc: 0.8603\n",
      "    > Eval: L=3, F=16... Acc: 0.8695\n",
      "    > Eval: L=3, F=30... Acc: 0.8002\n",
      "    > Eval: L=3, F=24... Acc: 0.8683\n",
      "\n",
      "Iter 3/8 | Best Acc: 0.8766 | W=0.775 | Success Rate=0.00\n",
      "    > Eval: L=2, F=39... Acc: 0.8647\n",
      "    > Eval: L=3, F=29... Acc: 0.8591\n",
      "    > Eval: L=3, F=16... Acc: 0.8607\n",
      "    > Eval: L=3, F=29... Acc: 0.8456\n",
      "    > Eval: L=3, F=24... Acc: 0.8719\n",
      "\n",
      "Iter 4/8 | Best Acc: 0.8766 | W=0.713 | Success Rate=0.00\n",
      "    > Eval: L=4, F=20... Acc: 0.8739\n",
      "    > Eval: L=3, F=29... Acc: 0.8496\n",
      "    > Eval: L=3, F=16... Acc: 0.8448\n",
      "    > Eval: L=3, F=29... Acc: 0.8703\n",
      "    > Eval: L=3, F=24... Acc: 0.8591\n",
      "\n",
      "Iter 5/8 | Best Acc: 0.8766 | W=0.650 | Success Rate=0.00\n",
      "    > Eval: L=4, F=23... Acc: 0.8595\n",
      "    > Eval: L=3, F=29... Acc: 0.8611\n",
      "    > Eval: L=3, F=16... Acc: 0.8611\n",
      "    > Eval: L=3, F=28... Acc: 0.8711\n",
      "    > Eval: L=3, F=24... Acc: 0.8575\n",
      "\n",
      "Iter 6/8 | Best Acc: 0.8766 | W=0.588 | Success Rate=0.00\n",
      "    > Eval: L=3, F=25... Acc: 0.8567\n",
      "    > Eval: L=3, F=29... Acc: 0.8575\n",
      "    > Eval: L=3, F=27... Acc: 0.8615\n",
      "    > Eval: L=3, F=29... Acc: 0.8647\n",
      "    > Eval: L=3, F=24... Acc: 0.8671\n",
      "\n",
      "Iter 7/8 | Best Acc: 0.8766 | W=0.525 | Success Rate=0.00\n",
      "    > Eval: L=3, F=23... Acc: 0.8731\n",
      "    > Eval: L=3, F=29... Acc: 0.8480\n",
      "    > Eval: L=3, F=35... Acc: 0.8739\n",
      "    > Eval: L=3, F=29... Acc: 0.8472\n",
      "    > Eval: L=3, F=24... Acc: 0.8464\n",
      "\n",
      "Iter 8/8 | Best Acc: 0.8766 | W=0.463 | Success Rate=0.00\n",
      "    > Eval: L=3, F=22... Acc: 0.8695\n",
      "    > Eval: L=3, F=29... Acc: 0.8719\n",
      "    > Eval: L=3, F=32... Acc: 0.8675\n",
      "    > Eval: L=3, F=28... Acc: 0.8241\n",
      "    > Eval: L=3, F=24... Acc: 0.8750\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== PSO with W Strategy: LINEAR_INCREASE ===\n",
      "    > Eval: L=2, F=105... Acc: 0.8663\n",
      "    > Eval: L=3, F=64... Acc: 0.8675\n",
      "    > Eval: L=2, F=82... Acc: 0.8424\n",
      "    > Eval: L=1, F=106... Acc: 0.7971\n",
      "    > Eval: L=1, F=108... Acc: 0.8432\n",
      "\n",
      "Iter 1/8 | Best Acc: 0.8675 | W=0.400 | Success Rate=1.00\n",
      "    > Eval: L=2, F=24... Acc: 0.8583\n",
      "    > Eval: L=3, F=64... Acc: 0.8699\n",
      "    > Eval: L=2, F=66... Acc: 0.8587\n",
      "    > Eval: L=2, F=54... Acc: 0.8647\n",
      "    > Eval: L=1, F=108... Acc: 0.8353\n",
      "\n",
      "Iter 2/8 | Best Acc: 0.8699 | W=0.463 | Success Rate=0.00\n",
      "    > Eval: L=2, F=119... Acc: 0.8635\n",
      "    > Eval: L=3, F=64... Acc: 0.8651\n",
      "    > Eval: L=3, F=57... Acc: 0.8659\n",
      "    > Eval: L=3, F=45... Acc: 0.8647\n",
      "    > Eval: L=1, F=108... Acc: 0.6948\n",
      "\n",
      "Iter 3/8 | Best Acc: 0.8699 | W=0.525 | Success Rate=0.00\n",
      "    > Eval: L=3, F=94... Acc: 0.8703\n",
      "    > Eval: L=3, F=73... Acc: 0.8731\n",
      "    > Eval: L=3, F=53... Acc: 0.8376\n",
      "    > Eval: L=3, F=94... Acc: 0.8567\n",
      "    > Eval: L=1, F=108... Acc: 0.8086\n",
      "\n",
      "Iter 4/8 | Best Acc: 0.8731 | W=0.588 | Success Rate=0.00\n",
      "    > Eval: L=4, F=55... Acc: 0.8257\n",
      "    > Eval: L=3, F=79... Acc: 0.8428\n",
      "    > Eval: L=3, F=65... Acc: 0.8651\n",
      "    > Eval: L=1, F=29... Acc: 0.8337\n",
      "    > Eval: L=1, F=108... Acc: 0.8337\n",
      "\n",
      "Iter 5/8 | Best Acc: 0.8731 | W=0.650 | Success Rate=0.00\n",
      "    > Eval: L=4, F=65... Acc: 0.8679\n",
      "    > Eval: L=3, F=68... Acc: 0.8273\n",
      "    > Eval: L=2, F=77... Acc: 0.8524\n",
      "    > Eval: L=3, F=92... Acc: 0.8404\n",
      "    > Eval: L=1, F=108... Acc: 0.6900\n",
      "\n",
      "Iter 6/8 | Best Acc: 0.8731 | W=0.713 | Success Rate=0.00\n",
      "    > Eval: L=2, F=109... Acc: 0.8496\n",
      "    > Eval: L=3, F=69... Acc: 0.8607\n",
      "    > Eval: L=3, F=83... Acc: 0.8611\n",
      "    > Eval: L=4, F=103... Acc: 0.8754\n",
      "    > Eval: L=1, F=108... Acc: 0.7688\n",
      "\n",
      "Iter 7/8 | Best Acc: 0.8754 | W=0.775 | Success Rate=0.00\n",
      "    > Eval: L=4, F=128... Acc: 0.8134\n",
      "    > Eval: L=3, F=128... Acc: 0.8647\n",
      "    > Eval: L=4, F=40... Acc: 0.8691\n",
      "    > Eval: L=4, F=111... Acc: 0.8591\n",
      "    > Eval: L=1, F=108... Acc: 0.8655\n",
      "\n",
      "Iter 8/8 | Best Acc: 0.8754 | W=0.838 | Success Rate=0.00\n",
      "    > Eval: L=4, F=96... Acc: 0.8345\n",
      "    > Eval: L=3, F=51... Acc: 0.8743\n",
      "    > Eval: L=4, F=26... Acc: 0.8575\n",
      "    > Eval: L=3, F=97... Acc: 0.8683\n",
      "    > Eval: L=1, F=108... Acc: 0.8516\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== PSO with W Strategy: RANDOM ===\n",
      "    > Eval: L=2, F=115... Acc: 0.8659\n",
      "    > Eval: L=2, F=118... Acc: 0.8305\n",
      "    > Eval: L=2, F=42... Acc: 0.8595\n",
      "    > Eval: L=2, F=128... Acc: 0.8229\n",
      "    > Eval: L=4, F=98... Acc: 0.8607\n",
      "\n",
      "Iter 1/8 | Best Acc: 0.8659 | W=0.835 | Success Rate=1.00\n",
      "    > Eval: L=2, F=115... Acc: 0.8094\n",
      "    > Eval: L=2, F=112... Acc: 0.8575\n",
      "    > Eval: L=2, F=117... Acc: 0.7979\n",
      "    > Eval: L=1, F=116... Acc: 0.7469\n",
      "    > Eval: L=4, F=98... Acc: 0.8106\n",
      "\n",
      "Iter 2/8 | Best Acc: 0.8659 | W=0.852 | Success Rate=0.00\n",
      "    > Eval: L=2, F=115... Acc: 0.8480\n",
      "    > Eval: L=2, F=111... Acc: 0.8711\n",
      "    > Eval: L=2, F=128... Acc: 0.8416\n",
      "    > Eval: L=2, F=118... Acc: 0.8496\n",
      "    > Eval: L=4, F=98... Acc: 0.7784\n",
      "\n",
      "Iter 3/8 | Best Acc: 0.8711 | W=0.527 | Success Rate=0.00\n",
      "    > Eval: L=2, F=112... Acc: 0.8305\n",
      "    > Eval: L=2, F=111... Acc: 0.8647\n",
      "    > Eval: L=2, F=55... Acc: 0.8655\n",
      "    > Eval: L=2, F=114... Acc: 0.8743\n",
      "    > Eval: L=4, F=98... Acc: 0.8631\n",
      "\n",
      "Iter 4/8 | Best Acc: 0.8743 | W=0.797 | Success Rate=0.00\n",
      "    > Eval: L=2, F=114... Acc: 0.8492\n",
      "    > Eval: L=2, F=112... Acc: 0.8659\n",
      "    > Eval: L=2, F=26... Acc: 0.8468\n",
      "    > Eval: L=2, F=111... Acc: 0.8627\n",
      "    > Eval: L=4, F=98... Acc: 0.8571\n",
      "\n",
      "Iter 5/8 | Best Acc: 0.8743 | W=0.858 | Success Rate=0.00\n",
      "    > Eval: L=2, F=117... Acc: 0.6140\n",
      "    > Eval: L=2, F=115... Acc: 0.7652\n",
      "    > Eval: L=2, F=62... Acc: 0.8229\n",
      "    > Eval: L=2, F=114... Acc: 0.8249\n",
      "    > Eval: L=4, F=98... Acc: 0.8452\n",
      "\n",
      "Iter 6/8 | Best Acc: 0.8743 | W=0.743 | Success Rate=0.00\n",
      "    > Eval: L=2, F=114... Acc: 0.8643\n",
      "    > Eval: L=1, F=112... Acc: 0.8297\n",
      "    > Eval: L=2, F=128... Acc: 0.8559\n",
      "    > Eval: L=2, F=117... Acc: 0.8762\n",
      "    > Eval: L=4, F=98... Acc: 0.8671\n",
      "\n",
      "Iter 7/8 | Best Acc: 0.8762 | W=0.573 | Success Rate=0.00\n",
      "    > Eval: L=2, F=117... Acc: 0.8444\n",
      "    > Eval: L=2, F=119... Acc: 0.8663\n",
      "    > Eval: L=2, F=115... Acc: 0.8707\n",
      "    > Eval: L=2, F=119... Acc: 0.8643\n",
      "    > Eval: L=4, F=98... Acc: 0.8663\n",
      "\n",
      "Iter 8/8 | Best Acc: 0.8762 | W=0.609 | Success Rate=0.00\n",
      "    > Eval: L=2, F=118... Acc: 0.8544\n",
      "    > Eval: L=2, F=120... Acc: 0.8623\n",
      "    > Eval: L=2, F=108... Acc: 0.8655\n",
      "    > Eval: L=2, F=118... Acc: 0.8635\n",
      "    > Eval: L=4, F=98... Acc: 0.8623\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== PSO with W Strategy: SUCCESS_RATE ===\n",
      "    > Eval: L=2, F=98... Acc: 0.8611\n",
      "    > Eval: L=1, F=71... Acc: 0.8556\n",
      "    > Eval: L=2, F=23... Acc: 0.8552\n",
      "    > Eval: L=2, F=42... Acc: 0.8663\n",
      "    > Eval: L=1, F=37... Acc: 0.8619\n",
      "\n",
      "Iter 1/8 | Best Acc: 0.8663 | W=0.000 | Success Rate=1.00\n",
      "    > Eval: L=3, F=62... Acc: 0.8703\n",
      "    > Eval: L=3, F=56... Acc: 0.8723\n",
      "    > Eval: L=4, F=24... Acc: 0.8536\n",
      "    > Eval: L=3, F=44... Acc: 0.8424\n",
      "    > Eval: L=1, F=37... Acc: 0.8440\n",
      "\n",
      "Iter 2/8 | Best Acc: 0.8723 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=4, F=57... Acc: 0.8548\n",
      "    > Eval: L=3, F=56... Acc: 0.8217\n",
      "    > Eval: L=1, F=71... Acc: 0.8086\n",
      "    > Eval: L=2, F=49... Acc: 0.8659\n",
      "    > Eval: L=1, F=37... Acc: 0.8659\n",
      "\n",
      "Iter 3/8 | Best Acc: 0.8723 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=1, F=60... Acc: 0.8599\n",
      "    > Eval: L=3, F=56... Acc: 0.7990\n",
      "    > Eval: L=3, F=16... Acc: 0.8372\n",
      "    > Eval: L=4, F=47... Acc: 0.8587\n",
      "    > Eval: L=1, F=37... Acc: 0.8528\n",
      "\n",
      "Iter 4/8 | Best Acc: 0.8723 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=3, F=56... Acc: 0.8719\n",
      "    > Eval: L=3, F=56... Acc: 0.8635\n",
      "    > Eval: L=2, F=99... Acc: 0.8563\n",
      "    > Eval: L=1, F=50... Acc: 0.8305\n",
      "    > Eval: L=1, F=37... Acc: 0.8544\n",
      "\n",
      "Iter 5/8 | Best Acc: 0.8723 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=3, F=56... Acc: 0.8229\n",
      "    > Eval: L=3, F=56... Acc: 0.8166\n",
      "    > Eval: L=2, F=67... Acc: 0.8384\n",
      "    > Eval: L=4, F=36... Acc: 0.8683\n",
      "    > Eval: L=1, F=37... Acc: 0.8571\n",
      "\n",
      "Iter 6/8 | Best Acc: 0.8723 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=3, F=56... Acc: 0.8711\n",
      "    > Eval: L=3, F=56... Acc: 0.8583\n",
      "    > Eval: L=2, F=64... Acc: 0.8651\n",
      "    > Eval: L=3, F=59... Acc: 0.8627\n",
      "    > Eval: L=1, F=37... Acc: 0.8631\n",
      "\n",
      "Iter 7/8 | Best Acc: 0.8723 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=4, F=55... Acc: 0.8615\n",
      "    > Eval: L=3, F=56... Acc: 0.8432\n",
      "    > Eval: L=4, F=49... Acc: 0.8655\n",
      "    > Eval: L=4, F=26... Acc: 0.8579\n",
      "    > Eval: L=1, F=37... Acc: 0.8556\n",
      "\n",
      "Iter 8/8 | Best Acc: 0.8723 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=2, F=56... Acc: 0.8607\n",
      "    > Eval: L=3, F=56... Acc: 0.8655\n",
      "    > Eval: L=3, F=54... Acc: 0.8380\n",
      "    > Eval: L=4, F=53... Acc: 0.8675\n",
      "    > Eval: L=1, F=37... Acc: 0.8134\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== PSO with W Strategy: RANK_BASED ===\n",
      "    > Eval: L=3, F=126... Acc: 0.8639\n",
      "    > Eval: L=2, F=118... Acc: 0.8623\n",
      "    > Eval: L=1, F=98... Acc: 0.8727\n",
      "    > Eval: L=3, F=121... Acc: 0.8500\n",
      "    > Eval: L=1, F=58... Acc: 0.5575\n",
      "\n",
      "Iter 1/8 | Best Acc: 0.8727 | W=0.700 | Success Rate=1.00\n",
      "    > Eval: L=1, F=103... Acc: 0.8627\n",
      "    > Eval: L=2, F=111... Acc: 0.4433\n",
      "    > Eval: L=1, F=98... Acc: 0.8762\n",
      "    > Eval: L=2, F=87... Acc: 0.8540\n",
      "    > Eval: L=1, F=58... Acc: 0.8365\n",
      "\n",
      "Iter 2/8 | Best Acc: 0.8762 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=3, F=97... Acc: 0.8651\n",
      "    > Eval: L=1, F=99... Acc: 0.8691\n",
      "    > Eval: L=1, F=98... Acc: 0.8691\n",
      "    > Eval: L=1, F=75... Acc: 0.8675\n",
      "    > Eval: L=1, F=58... Acc: 0.8345\n",
      "\n",
      "Iter 3/8 | Best Acc: 0.8762 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=4, F=96... Acc: 0.8619\n",
      "    > Eval: L=1, F=94... Acc: 0.8619\n",
      "    > Eval: L=1, F=98... Acc: 0.8066\n",
      "    > Eval: L=1, F=102... Acc: 0.8524\n",
      "    > Eval: L=1, F=58... Acc: 0.8595\n",
      "\n",
      "Iter 4/8 | Best Acc: 0.8762 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=2, F=102... Acc: 0.8683\n",
      "    > Eval: L=1, F=103... Acc: 0.6255\n",
      "    > Eval: L=1, F=98... Acc: 0.8524\n",
      "    > Eval: L=1, F=79... Acc: 0.8615\n",
      "    > Eval: L=1, F=58... Acc: 0.8170\n",
      "\n",
      "Iter 5/8 | Best Acc: 0.8762 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=1, F=99... Acc: 0.7863\n",
      "    > Eval: L=1, F=100... Acc: 0.8329\n",
      "    > Eval: L=1, F=98... Acc: 0.6442\n",
      "    > Eval: L=1, F=76... Acc: 0.8181\n",
      "    > Eval: L=1, F=58... Acc: 0.8528\n",
      "\n",
      "Iter 6/8 | Best Acc: 0.8762 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=1, F=99... Acc: 0.7807\n",
      "    > Eval: L=1, F=96... Acc: 0.8667\n",
      "    > Eval: L=1, F=98... Acc: 0.8496\n",
      "    > Eval: L=1, F=77... Acc: 0.8305\n",
      "    > Eval: L=1, F=58... Acc: 0.8536\n",
      "\n",
      "Iter 7/8 | Best Acc: 0.8762 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=3, F=103... Acc: 0.8372\n",
      "    > Eval: L=1, F=102... Acc: 0.8623\n",
      "    > Eval: L=1, F=98... Acc: 0.7752\n",
      "    > Eval: L=1, F=88... Acc: 0.8193\n",
      "    > Eval: L=1, F=58... Acc: 0.8607\n",
      "\n",
      "Iter 8/8 | Best Acc: 0.8762 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=1, F=102... Acc: 0.8392\n",
      "    > Eval: L=1, F=98... Acc: 0.8269\n",
      "    > Eval: L=1, F=98... Acc: 0.8635\n",
      "    > Eval: L=1, F=102... Acc: 0.8583\n",
      "    > Eval: L=1, F=58... Acc: 0.8237\n",
      "\n",
      "====================================================================================================\n",
      "COMPARISON OF INERTIA WEIGHT STRATEGIES\n",
      "====================================================================================================\n",
      "STRATEGY             | LAYERS | FILTERS | ACCURACY   | TIME(s)  | AVG W   | W STD\n",
      "----------------------------------------------------------------------------------------------------\n",
      "linear_decrease      | 3      | 29      | 87.66    % | 924.8    | 0.681   | 0.143\n",
      "linear_increase      | 4      | 103     | 87.54    % | 935.4    | 0.619   | 0.143\n",
      "random               | 2      | 117     | 87.62    % | 979.7    | 0.724   | 0.126\n",
      "success_rate         | 3      | 56      | 87.23    % | 875.0    | 0.000   | 0.000\n",
      "rank_based           | 1      | 98      | 87.62    % | 920.2    | 0.700   | 0.000\n",
      "====================================================================================================\n",
      "\n",
      "🏆 BEST STRATEGY: LINEAR_DECREASE\n",
      "   Best Accuracy: 87.66%\n",
      "   Parameters: L=3, F=29\n"
     ]
    }
   ],
   "source": [
    "# Classification_Task3_W_Strategies.py\n",
    "# Compare multiple inertia weight strategies from the paper\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    DATASET_PATH = \"/kaggle/input/waste-classification-data/DATASET\"\n",
    "    IMG_SIZE = (128, 128)\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_CLASSES = 2\n",
    "    TRAIN_DATA_PERCENTAGE = 0.25\n",
    "\n",
    "    POPULATION_SIZE = 5\n",
    "    MAX_ITERATIONS = 8\n",
    "    C1 = 2.0\n",
    "    C2 = 2.0\n",
    "    W_MAX = 1.0\n",
    "    W_MIN = 0.0\n",
    "\n",
    "    MIN_LAYERS = 1\n",
    "    MAX_LAYERS = 4\n",
    "    MIN_FILTERS = 16\n",
    "    MAX_FILTERS = 128\n",
    "\n",
    "    EVAL_EPOCHS = 1\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA LOADING\n",
    "# ==========================================\n",
    "def get_data_loaders():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(Config.IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_dir = os.path.join(Config.DATASET_PATH, 'TRAIN')\n",
    "    test_dir = os.path.join(Config.DATASET_PATH, 'TEST')\n",
    "\n",
    "    try:\n",
    "        train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "        test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "        \n",
    "        if Config.TRAIN_DATA_PERCENTAGE < 1.0:\n",
    "            indices = list(range(len(train_dataset)))\n",
    "            split = int(np.floor(Config.TRAIN_DATA_PERCENTAGE * len(train_dataset)))\n",
    "            subset_indices = random.sample(indices, split)\n",
    "            train_dataset = Subset(train_dataset, subset_indices)\n",
    "            print(f\"  [Info] Using {len(train_dataset)} images for training.\")\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "        return train_loader, test_loader\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==========================================\n",
    "# 3. MODEL\n",
    "# ==========================================\n",
    "class DynamicCNN(nn.Module):\n",
    "    def __init__(self, num_layers, num_filters):\n",
    "        super(DynamicCNN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_channels = 3\n",
    "        \n",
    "        for _ in range(int(num_layers)):\n",
    "            self.convs.append(nn.Conv2d(in_channels, int(num_filters), 3, 1, 1))\n",
    "            self.convs.append(nn.MaxPool2d(2, 2))\n",
    "            in_channels = int(num_filters)\n",
    "        \n",
    "        final_dim = Config.IMG_SIZE[0]\n",
    "        for _ in range(int(num_layers)):\n",
    "            final_dim = final_dim // 2\n",
    "        if final_dim < 1:\n",
    "            final_dim = 1\n",
    "            \n",
    "        self.flatten_size = in_channels * final_dim * final_dim\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
    "        self.fc2 = nn.Linear(64, Config.NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.convs:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        x = x.view(-1, self.flatten_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ==========================================\n",
    "# 4. INERTIA WEIGHT STRATEGIES (FROM PAPER)\n",
    "# ==========================================\n",
    "class InertiaWeightStrategy:\n",
    "    \"\"\"\n",
    "    Implements W strategies from:\n",
    "    \"A novel particle swarm optimization algorithm with adaptive inertia weight\"\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_decreasing(iteration, max_iter, w_max=0.9, w_min=0.4):\n",
    "        \"\"\"W3: Standard linear decrease\"\"\"\n",
    "        return w_max - ((w_max - w_min) * iteration / max_iter)\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_increasing(iteration, max_iter, w_max=0.9, w_min=0.4):\n",
    "        \"\"\"W9: Linear increase\"\"\"\n",
    "        return w_min + ((w_max - w_min) * iteration / max_iter)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_inertia():\n",
    "        \"\"\"W2: Random inertia weight\"\"\"\n",
    "        return 0.5 + np.random.rand() / 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def success_rate_adaptive(success_rate, w_max=1.0, w_min=0.0):\n",
    "        \"\"\"\n",
    "        AIWPSO (Equation 20 from paper):\n",
    "        w(t) = (w_max - w_min) * Ps(t) + w_min\n",
    "        \"\"\"\n",
    "        return (w_max - w_min) * success_rate + w_min\n",
    "    \n",
    "    @staticmethod\n",
    "    def rank_based(particle_rank, total_population, w_max=0.9, w_min=0.4):\n",
    "        \"\"\"\n",
    "        W13: Rank-based inertia weight\n",
    "        Better particles get lower w, worse particles get higher w\n",
    "        \"\"\"\n",
    "        return w_min + (w_max - w_min) * (particle_rank / total_population)\n",
    "\n",
    "# ==========================================\n",
    "# 5. PSO WITH MULTIPLE W STRATEGIES\n",
    "# ==========================================\n",
    "class AdaptivePSO:\n",
    "    def __init__(self, train_loader, test_loader, w_strategy='linear_decrease'):\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.w_strategy = w_strategy\n",
    "        \n",
    "        self.positions = np.zeros((Config.POPULATION_SIZE, 2))\n",
    "        self.positions[:, 0] = np.random.uniform(Config.MIN_LAYERS, Config.MAX_LAYERS, Config.POPULATION_SIZE)\n",
    "        self.positions[:, 1] = np.random.uniform(Config.MIN_FILTERS, Config.MAX_FILTERS, Config.POPULATION_SIZE)\n",
    "        \n",
    "        self.velocities = np.zeros((Config.POPULATION_SIZE, 2))\n",
    "        self.pbest_pos = self.positions.copy()\n",
    "        self.pbest_val = np.zeros(Config.POPULATION_SIZE)\n",
    "        self.gbest_pos = np.zeros(2)\n",
    "        self.gbest_val = 0.0\n",
    "        \n",
    "        self.prev_pbest_val = np.zeros(Config.POPULATION_SIZE)\n",
    "        self.w_history = []\n",
    "\n",
    "    def evaluate(self, pos):\n",
    "        L = int(np.clip(np.round(pos[0]), Config.MIN_LAYERS, Config.MAX_LAYERS))\n",
    "        F_num = int(np.clip(np.round(pos[1]), Config.MIN_FILTERS, Config.MAX_FILTERS))\n",
    "        \n",
    "        print(f\"    > Eval: L={L}, F={F_num}...\", end=\" \")\n",
    "        \n",
    "        try:\n",
    "            model = DynamicCNN(L, F_num).to(Config.DEVICE)\n",
    "            opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "            \n",
    "            model.train()\n",
    "            for img, lbl in self.train_loader:\n",
    "                opt.zero_grad()\n",
    "                F.cross_entropy(model(img.to(Config.DEVICE)), lbl.to(Config.DEVICE)).backward()\n",
    "                opt.step()\n",
    "            \n",
    "            model.eval()\n",
    "            corr, tot = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for img, lbl in self.test_loader:\n",
    "                    out = model(img.to(Config.DEVICE))\n",
    "                    corr += (out.argmax(1) == lbl.to(Config.DEVICE)).sum().item()\n",
    "                    tot += lbl.size(0)\n",
    "            \n",
    "            acc = corr / tot\n",
    "            print(f\"Acc: {acc:.4f}\")\n",
    "            return acc\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def calculate_success_rate(self):\n",
    "        \"\"\"Equation 17-18 from paper\"\"\"\n",
    "        successes = 0\n",
    "        for i in range(Config.POPULATION_SIZE):\n",
    "            if self.pbest_val[i] > self.prev_pbest_val[i]:  # Maximization\n",
    "                successes += 1\n",
    "        return successes / Config.POPULATION_SIZE\n",
    "\n",
    "    def get_inertia_weight(self, iteration, particle_idx=None, fitness=None):\n",
    "        if self.w_strategy == 'linear_decrease':\n",
    "            return InertiaWeightStrategy.linear_decreasing(iteration, Config.MAX_ITERATIONS)\n",
    "        \n",
    "        elif self.w_strategy == 'linear_increase':\n",
    "            return InertiaWeightStrategy.linear_increasing(iteration, Config.MAX_ITERATIONS)\n",
    "        \n",
    "        elif self.w_strategy == 'random':\n",
    "            return InertiaWeightStrategy.random_inertia()\n",
    "        \n",
    "        elif self.w_strategy == 'success_rate':\n",
    "            success_rate = self.calculate_success_rate()\n",
    "            return InertiaWeightStrategy.success_rate_adaptive(success_rate)\n",
    "        \n",
    "        elif self.w_strategy == 'rank_based':\n",
    "            if fitness is not None and particle_idx is not None:\n",
    "                ranks = np.argsort(np.argsort(-fitness))  # Higher is better\n",
    "                return InertiaWeightStrategy.rank_based(ranks[particle_idx], Config.POPULATION_SIZE)\n",
    "            return 0.7\n",
    "        \n",
    "        return 0.7\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"\\n=== PSO with W Strategy: {self.w_strategy.upper()} ===\")\n",
    "        \n",
    "        fitness = np.array([self.evaluate(p) for p in self.positions])\n",
    "        self.pbest_val = fitness.copy()\n",
    "        self.prev_pbest_val = fitness.copy()\n",
    "        best_idx = fitness.argmax()\n",
    "        self.gbest_val = fitness[best_idx]\n",
    "        self.gbest_pos = self.positions[best_idx].copy()\n",
    "        \n",
    "        for iteration in range(Config.MAX_ITERATIONS):\n",
    "            self.prev_pbest_val = self.pbest_val.copy()\n",
    "            base_w = self.get_inertia_weight(iteration, fitness=fitness)\n",
    "            \n",
    "            if iteration > 0:\n",
    "                success_rate = self.calculate_success_rate()\n",
    "            else:\n",
    "                success_rate = 1.0\n",
    "            \n",
    "            self.w_history.append(base_w)\n",
    "            \n",
    "            print(f\"\\nIter {iteration+1}/{Config.MAX_ITERATIONS} | Best Acc: {self.gbest_val:.4f} | W={base_w:.3f} | Success Rate={success_rate:.2f}\")\n",
    "            \n",
    "            for i in range(Config.POPULATION_SIZE):\n",
    "                if self.w_strategy == 'rank_based':\n",
    "                    w = self.get_inertia_weight(iteration, particle_idx=i, fitness=fitness)\n",
    "                else:\n",
    "                    w = base_w\n",
    "                \n",
    "                r1, r2 = np.random.rand(2), np.random.rand(2)\n",
    "                social_term = 0 if i == Config.POPULATION_SIZE - 1 else Config.C2 * r2 * (self.gbest_pos - self.positions[i])\n",
    "                cognitive_term = Config.C1 * r1 * (self.pbest_pos[i] - self.positions[i])\n",
    "                \n",
    "                self.velocities[i] = w * self.velocities[i] + cognitive_term + social_term\n",
    "                self.positions[i] += self.velocities[i]\n",
    "                \n",
    "                self.positions[i, 0] = np.clip(self.positions[i, 0], Config.MIN_LAYERS, Config.MAX_LAYERS)\n",
    "                self.positions[i, 1] = np.clip(self.positions[i, 1], Config.MIN_FILTERS, Config.MAX_FILTERS)\n",
    "                \n",
    "                acc = self.evaluate(self.positions[i])\n",
    "                fitness[i] = acc\n",
    "                \n",
    "                if acc > self.pbest_val[i]:\n",
    "                    self.pbest_val[i] = acc\n",
    "                    self.pbest_pos[i] = self.positions[i].copy()\n",
    "                if acc > self.gbest_val:\n",
    "                    self.gbest_val = acc\n",
    "                    self.gbest_pos = self.positions[i].copy()\n",
    "        \n",
    "        return self.gbest_pos, self.gbest_val, self.w_history\n",
    "\n",
    "# ==========================================\n",
    "# 6. MAIN - COMPARE ALL STRATEGIES\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(\"=\" * 100)\n",
    "    print(\"CLASSIFICATION TASK 3: Compare Inertia Weight Strategies\")\n",
    "    print(\"Based on: 'A novel PSO algorithm with adaptive inertia weight'\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    train_loader, test_loader = get_data_loaders()\n",
    "    if train_loader is None:\n",
    "        return\n",
    "    \n",
    "    strategies = [\n",
    "        'linear_decrease',\n",
    "        'linear_increase',\n",
    "        'random',\n",
    "        'success_rate',\n",
    "        'rank_based'\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        start = time.time()\n",
    "        pso = AdaptivePSO(train_loader, test_loader, w_strategy=strategy)\n",
    "        best_pos, best_acc, w_history = pso.run()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        results.append({\n",
    "            'strategy': strategy,\n",
    "            'layers': int(np.round(best_pos[0])),\n",
    "            'filters': int(np.round(best_pos[1])),\n",
    "            'accuracy': best_acc,\n",
    "            'time': elapsed,\n",
    "            'avg_w': np.mean(w_history),\n",
    "            'w_std': np.std(w_history)\n",
    "        })\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"COMPARISON OF INERTIA WEIGHT STRATEGIES\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'STRATEGY':<20} | {'LAYERS':<6} | {'FILTERS':<7} | {'ACCURACY':<10} | {'TIME(s)':<8} | {'AVG W':<7} | {'W STD'}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r['strategy']:<20} | {r['layers']:<6} | {r['filters']:<7} | {r['accuracy']*100:<9.2f}% | {r['time']:<8.1f} | {r['avg_w']:<7.3f} | {r['w_std']:.3f}\")\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    best = max(results, key=lambda x: x['accuracy'])\n",
    "    print(f\"\\n🏆 BEST STRATEGY: {best['strategy'].upper()}\")\n",
    "    print(f\"   Best Accuracy: {best['accuracy']*100:.2f}%\")\n",
    "    print(f\"   Parameters: L={best['layers']}, F={best['filters']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 233210,
     "sourceId": 497253,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
