{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-14T20:54:37.162617Z",
     "iopub.status.busy": "2025-12-14T20:54:37.161808Z",
     "iopub.status.idle": "2025-12-14T20:58:45.561581Z",
     "shell.execute_reply": "2025-12-14T20:58:45.560705Z",
     "shell.execute_reply.started": "2025-12-14T20:54:37.162581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TASK 1: LLM Controls c1/c2 + Particle Position/Velocity History\n",
      "======================================================================\n",
      "Downloading dataset...\n",
      "\n",
      "=== Starting LLM-Controlled PSO (c1/c2 + Particle History) ===\n",
      "    > Eval: L=1, N=150... RMSE: 0.02816\n",
      "    > Eval: L=1, N=179... RMSE: 0.02312\n",
      "    > Eval: L=5, N=16... RMSE: 0.03346\n",
      "    > Eval: L=2, N=202... RMSE: 0.02222\n",
      "\n",
      "Iter 1/10 | Best RMSE: 0.02222 | c1=2.00, c2=2.00\n",
      "    > Eval: L=1, N=213... RMSE: 0.02355\n",
      "    > Eval: L=2, N=202... RMSE: 0.02161\n",
      "    > Eval: L=3, N=256... RMSE: 0.02203\n",
      "    > Eval: L=2, N=202... RMSE: 0.02593\n",
      "\n",
      "Iter 2/10 | Best RMSE: 0.02161 | c1=2.00, c2=2.00\n",
      "    > Eval: L=2, N=256... RMSE: 0.02303\n",
      "    > Eval: L=2, N=222... RMSE: 0.02169\n",
      "    > Eval: L=1, N=256... RMSE: 0.02289\n",
      "    > Eval: L=2, N=202... RMSE: 0.02150\n",
      "\n",
      "Iter 3/10 | Best RMSE: 0.02150 | c1=2.00, c2=2.00\n",
      "  [LLM] Consulting for suggestions + c1/c2 tuning...\n",
      "  [LLM Analysis]: Velocities, especially in neuron dimension, are large and cause boundary hits and oscillation around the best region, indicating overshooting rather than stagnation.\n",
      "  [LLM] Updating: c1=2.00->1.60, c2=2.00->1.60\n",
      "  [LLM] Replacing P2 -> [2, 210]\n",
      "    > Eval: L=2, N=210... RMSE: 0.02174\n",
      "  [LLM] Replacing P0 -> [2, 230]\n",
      "    > Eval: L=2, N=230... RMSE: 0.02225\n",
      "    > Eval: L=2, N=224... RMSE: 0.02459\n",
      "    > Eval: L=2, N=207... RMSE: 0.02453\n",
      "    > Eval: L=3, N=252... RMSE: 0.02418\n",
      "    > Eval: L=2, N=202... RMSE: 0.02190\n",
      "\n",
      "Iter 4/10 | Best RMSE: 0.02150 | c1=1.60, c2=1.60\n",
      "    > Eval: L=2, N=186... RMSE: 0.02175\n",
      "    > Eval: L=2, N=186... RMSE: 0.02283\n",
      "    > Eval: L=3, N=219... RMSE: 0.02189\n",
      "    > Eval: L=2, N=202... RMSE: 0.02215\n",
      "\n",
      "Iter 5/10 | Best RMSE: 0.02150 | c1=1.60, c2=1.60\n",
      "    > Eval: L=2, N=164... RMSE: 0.02202\n",
      "    > Eval: L=2, N=209... RMSE: 0.02162\n",
      "    > Eval: L=3, N=190... RMSE: 0.02172\n",
      "    > Eval: L=2, N=202... RMSE: 0.02277\n",
      "\n",
      "Iter 6/10 | Best RMSE: 0.02150 | c1=1.60, c2=1.60\n",
      "  [LLM] Consulting for suggestions + c1/c2 tuning...\n",
      "  [LLM Analysis]: Velocities, especially on the neuron dimension, are large and cause oscillation around the best region rather than fine local search, so c1 and c2 should be slightly reduced to dampen overshooting.\n",
      "  [LLM] Updating: c1=1.60->1.40, c2=1.60->1.40\n",
      "  [LLM] Replacing P0 -> [2, 195]\n",
      "    > Eval: L=2, N=195... RMSE: 0.02436\n",
      "  [LLM] Replacing P3 -> [2, 210]\n",
      "    > Eval: L=2, N=210... RMSE: 0.02213\n",
      "    > Eval: L=3, N=189... RMSE: 0.02191\n",
      "    > Eval: L=2, N=217... RMSE: 0.02307\n",
      "    > Eval: L=2, N=185... RMSE: 0.02247\n",
      "    > Eval: L=2, N=208... RMSE: 0.02185\n",
      "\n",
      "Iter 7/10 | Best RMSE: 0.02150 | c1=1.40, c2=1.40\n",
      "    > Eval: L=3, N=196... RMSE: 0.02330\n",
      "    > Eval: L=2, N=191... RMSE: 0.02307\n",
      "    > Eval: L=2, N=188... RMSE: 0.02158\n",
      "    > Eval: L=2, N=202... RMSE: 0.02183\n",
      "\n",
      "Iter 8/10 | Best RMSE: 0.02150 | c1=1.40, c2=1.40\n",
      "    > Eval: L=2, N=203... RMSE: 0.02205\n",
      "    > Eval: L=2, N=183... RMSE: 0.02251\n",
      "    > Eval: L=2, N=203... RMSE: 0.02214\n",
      "    > Eval: L=2, N=198... RMSE: 0.02256\n",
      "\n",
      "Iter 9/10 | Best RMSE: 0.02150 | c1=1.40, c2=1.40\n",
      "  [LLM] Consulting for suggestions + c1/c2 tuning...\n",
      "  [LLM Analysis]: Velocities show moderate jumps in neurons (up to ~25) but positions remain clustered near the best region (L≈2, N≈190–205), indicating no severe overshooting or stagnation and suggesting only a mild adjustment is needed.\n",
      "  [LLM] Updating: c1=1.40->1.30, c2=1.40->1.30\n",
      "  [LLM] Replacing P1 -> [2, 196]\n",
      "    > Eval: L=2, N=196... RMSE: 0.02244\n",
      "  [LLM] Replacing P3 -> [2, 210]\n",
      "    > Eval: L=2, N=210... RMSE: 0.02281\n",
      "    > Eval: L=2, N=201... RMSE: 0.02256\n",
      "    > Eval: L=2, N=204... RMSE: 0.02213\n",
      "    > Eval: L=2, N=190... RMSE: 0.02158\n",
      "    > Eval: L=2, N=201... RMSE: 0.02265\n",
      "\n",
      "Iter 10/10 | Best RMSE: 0.02150 | c1=1.30, c2=1.30\n",
      "    > Eval: L=2, N=188... RMSE: 0.02181\n",
      "    > Eval: L=2, N=206... RMSE: 0.02253\n",
      "    > Eval: L=2, N=187... RMSE: 0.02236\n",
      "    > Eval: L=2, N=198... RMSE: 0.02169\n",
      "\n",
      "======================================================================\n",
      "RESULT: Best Params = [L=2, N=201]\n",
      "        Best RMSE = 0.02150\n",
      "        Time = 239.5s\n",
      "        Final c1=1.30, c2=1.30\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# PSO_regression_task1.py\n",
    "# Task 1: LLM controls c1, c2 + provides particle position/velocity history\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    AVALAI_API_KEY = \"\"\n",
    "    AVALAI_BASE_URL = \"https://api.avalai.ir/v1/chat/completions\"\n",
    "    LLM_MODEL = \"gpt-5.1\"\n",
    "\n",
    "    DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv\"\n",
    "    SEQ_LENGTH = 24\n",
    "    TRAIN_SPLIT = 0.8\n",
    "    BATCH_SIZE = 128\n",
    "\n",
    "    POPULATION_SIZE = 4\n",
    "    MAX_ITERATIONS = 10\n",
    "    \n",
    "    # Initial C1, C2 (will be updated by LLM)\n",
    "    C1_INIT = 2.0\n",
    "    C2_INIT = 2.0\n",
    "    \n",
    "    W_MAX = 0.9\n",
    "    W_MIN = 0.4\n",
    "\n",
    "    BOUNDS = {\n",
    "        'layers_min': 1, 'layers_max': 6,\n",
    "        'neurons_min': 16, 'neurons_max': 256\n",
    "    }\n",
    "\n",
    "    EPOCHS_EVAL = 2\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. ENHANCED MEMORY MODULE (with Position/Velocity History)\n",
    "# ==========================================\n",
    "class OptimizationMemory:\n",
    "    def __init__(self):\n",
    "        self.history = []  # Fitness history\n",
    "        self.particle_states = []  # Position/Velocity snapshots\n",
    "        self.c1_c2_history = []  # Track c1, c2 changes\n",
    "    \n",
    "    def add_record(self, layers, neurons, rmse):\n",
    "        self.history.append({\n",
    "            'params': [int(layers), int(neurons)],\n",
    "            'rmse': float(rmse)\n",
    "        })\n",
    "    \n",
    "    def add_particle_state(self, iteration, positions, velocities, fitness):\n",
    "        \"\"\"Store complete swarm state for LLM context\"\"\"\n",
    "        state = {\n",
    "            'iteration': iteration,\n",
    "            'particles': []\n",
    "        }\n",
    "        for i in range(len(positions)):\n",
    "            state['particles'].append({\n",
    "                'id': i,\n",
    "                'position': [round(positions[i][0], 2), round(positions[i][1], 2)],\n",
    "                'velocity': [round(velocities[i][0], 3), round(velocities[i][1], 3)],\n",
    "                'fitness': round(fitness[i], 5)\n",
    "            })\n",
    "        self.particle_states.append(state)\n",
    "        # Keep only last 3 states to avoid token overflow\n",
    "        if len(self.particle_states) > 3:\n",
    "            self.particle_states.pop(0)\n",
    "    \n",
    "    def add_c1_c2_record(self, c1, c2, iteration):\n",
    "        self.c1_c2_history.append({'iter': iteration, 'c1': c1, 'c2': c2})\n",
    "    \n",
    "    def get_context_string(self):\n",
    "        if not self.history:\n",
    "            return \"No history yet.\"\n",
    "        \n",
    "        sorted_hist = sorted(self.history, key=lambda x: x['rmse'])\n",
    "        best_3 = sorted_hist[:3]\n",
    "        worst_3 = sorted_hist[-3:]\n",
    "        \n",
    "        context = \"=== OPTIMIZATION MEMORY ===\\n\"\n",
    "        context += \"TOP 3 Configurations (Lowest RMSE):\\n\"\n",
    "        for i, h in enumerate(best_3):\n",
    "            context += f\"  {i+1}. [L={h['params'][0]}, N={h['params'][1]}] -> RMSE: {h['rmse']:.5f}\\n\"\n",
    "        \n",
    "        context += \"\\nWORST 3 Configurations:\\n\"\n",
    "        for i, h in enumerate(worst_3):\n",
    "            context += f\"  {i+1}. [L={h['params'][0]}, N={h['params'][1]}] -> RMSE: {h['rmse']:.5f}\\n\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def get_particle_state_string(self):\n",
    "        \"\"\"Return recent particle positions and velocities for LLM\"\"\"\n",
    "        if not self.particle_states:\n",
    "            return \"No particle state history.\"\n",
    "        \n",
    "        context = \"\\n=== PARTICLE STATE HISTORY (Last 3 Iterations) ===\\n\"\n",
    "        for state in self.particle_states:\n",
    "            context += f\"\\n--- Iteration {state['iteration']} ---\\n\"\n",
    "            for p in state['particles']:\n",
    "                context += (f\"  P{p['id']}: Pos=[L:{p['position'][0]:.1f}, N:{p['position'][1]:.0f}] \"\n",
    "                           f\"Vel=[{p['velocity'][0]:+.2f}, {p['velocity'][1]:+.2f}] \"\n",
    "                           f\"RMSE:{p['fitness']:.5f}\\n\")\n",
    "        return context\n",
    "    \n",
    "    def get_c1_c2_history_string(self):\n",
    "        if not self.c1_c2_history:\n",
    "            return \"No c1/c2 history.\"\n",
    "        context = \"\\n=== C1/C2 HISTORY ===\\n\"\n",
    "        for h in self.c1_c2_history[-5:]:\n",
    "            context += f\"  Iter {h['iter']}: c1={h['c1']:.2f}, c2={h['c2']:.2f}\\n\"\n",
    "        return context\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "def get_data():\n",
    "    print(f\"Downloading dataset...\")\n",
    "    try:\n",
    "        df = pd.read_csv(Config.DATASET_URL)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    cols = ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']\n",
    "    df = df[cols]\n",
    "    df['pm2.5'] = df['pm2.5'].ffill().bfill()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    values = df.values.astype('float32')\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled) - Config.SEQ_LENGTH):\n",
    "        X.append(scaled[i:i+Config.SEQ_LENGTH, :])\n",
    "        y.append(scaled[i+Config.SEQ_LENGTH, 0])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    split = int(len(X) * Config.TRAIN_SPLIT)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(torch.from_numpy(X[:split]), torch.from_numpy(y[:split])), \n",
    "                              batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(torch.from_numpy(X[split:]), torch.from_numpy(y[split:])), \n",
    "                             batch_size=Config.BATCH_SIZE)\n",
    "    \n",
    "    return train_loader, test_loader, X.shape[2], scaler\n",
    "\n",
    "# ==========================================\n",
    "# 4. MODEL\n",
    "# ==========================================\n",
    "class DynamicLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers, hidden_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = int(num_layers)\n",
    "        self.hidden_size = int(hidden_size)\n",
    "        self.lstm = nn.LSTM(input_dim, self.hidden_size, self.num_layers, \n",
    "                           batch_first=True, dropout=0.2 if self.num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(Config.DEVICE)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(Config.DEVICE)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "# ==========================================\n",
    "# 5. LLM INTERFACE (Enhanced: Returns c1, c2 + suggestions)\n",
    "# ==========================================\n",
    "def get_llm_suggestions_with_params(memory_context, particle_state_context, c1_c2_history, current_c1, current_c2):\n",
    "    \"\"\"\n",
    "    LLM now returns:\n",
    "    1. New hyperparameter suggestions\n",
    "    2. Recommended c1, c2 values\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in PSO Hyperparameter Optimization for Deep Learning.\n",
    "\n",
    "OBJECTIVE: Minimize RMSE for LSTM model.\n",
    "\n",
    "SEARCH SPACE:\n",
    "- Layers: {Config.BOUNDS['layers_min']} to {Config.BOUNDS['layers_max']}\n",
    "- Neurons: {Config.BOUNDS['neurons_min']} to {Config.BOUNDS['neurons_max']}\n",
    "\n",
    "CURRENT PSO PARAMETERS:\n",
    "- c1 (cognitive): {current_c1:.2f}\n",
    "- c2 (social): {current_c2:.2f}\n",
    "\n",
    "{memory_context}\n",
    "\n",
    "{particle_state_context}\n",
    "\n",
    "{c1_c2_history}\n",
    "\n",
    "=== YOUR TASKS ===\n",
    "\n",
    "1. ANALYZE the particle velocities:\n",
    "   - If velocities are too high (particles overshooting), suggest LOWER c1/c2.\n",
    "   - If velocities are too low (stagnation), suggest HIGHER c1/c2.\n",
    "   - Typical range: c1, c2 ∈ [1.0, 3.0]\n",
    "\n",
    "2. SUGGEST 2 new configurations [layers, neurons] based on the history.\n",
    "\n",
    "3. RECOMMEND new c1 and c2 values.\n",
    "\n",
    "OUTPUT FORMAT (JSON only, no extra text):\n",
    "{{\n",
    "    \"analysis\": \"Brief 1-sentence reasoning about velocity patterns...\",\n",
    "    \"suggestions\": [[layers1, neurons1], [layers2, neurons2]],\n",
    "    \"recommended_c1\": float,\n",
    "    \"recommended_c2\": float\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"  [LLM] Consulting for suggestions + c1/c2 tuning...\")\n",
    "        resp = requests.post(\n",
    "            Config.AVALAI_BASE_URL,\n",
    "            headers={\"Authorization\": f\"Bearer {Config.AVALAI_API_KEY}\", \"Content-Type\": \"application/json\"},\n",
    "            json={\"model\": Config.LLM_MODEL, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.5},\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            content = resp.json()['choices'][0]['message']['content']\n",
    "            content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            data = json.loads(content)\n",
    "            \n",
    "            print(f\"  [LLM Analysis]: {data.get('analysis', 'N/A')}\")\n",
    "            \n",
    "            suggestions = data.get(\"suggestions\", [])\n",
    "            new_c1 = float(data.get(\"recommended_c1\", current_c1))\n",
    "            new_c2 = float(data.get(\"recommended_c2\", current_c2))\n",
    "            \n",
    "            # Clamp c1, c2 to valid range\n",
    "            new_c1 = np.clip(new_c1, 1.0, 3.0)\n",
    "            new_c2 = np.clip(new_c2, 1.0, 3.0)\n",
    "            \n",
    "            return suggestions, new_c1, new_c2\n",
    "        else:\n",
    "            print(f\"  [LLM Error] Status: {resp.status_code}\")\n",
    "            return [], current_c1, current_c2\n",
    "    except Exception as e:\n",
    "        print(f\"  [LLM Failed]: {e}\")\n",
    "        return [], current_c1, current_c2\n",
    "\n",
    "# ==========================================\n",
    "# 6. PSO WITH LLM-CONTROLLED C1/C2\n",
    "# ==========================================\n",
    "class LLMControlledPSO:\n",
    "    def __init__(self, train_loader, test_loader, input_dim):\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.input_dim = input_dim\n",
    "        self.memory = OptimizationMemory()\n",
    "        \n",
    "        # Initialize swarm\n",
    "        self.positions = np.zeros((Config.POPULATION_SIZE, 2))\n",
    "        self.positions[:, 0] = np.random.uniform(Config.BOUNDS['layers_min'], Config.BOUNDS['layers_max'], Config.POPULATION_SIZE)\n",
    "        self.positions[:, 1] = np.random.uniform(Config.BOUNDS['neurons_min'], Config.BOUNDS['neurons_max'], Config.POPULATION_SIZE)\n",
    "        \n",
    "        self.velocities = np.zeros((Config.POPULATION_SIZE, 2))\n",
    "        self.pbest_pos = self.positions.copy()\n",
    "        self.pbest_val = np.full(Config.POPULATION_SIZE, float('inf'))\n",
    "        self.gbest_pos = np.zeros(2)\n",
    "        self.gbest_val = float('inf')\n",
    "        \n",
    "        # Dynamic c1, c2 (LLM will update these)\n",
    "        self.c1 = Config.C1_INIT\n",
    "        self.c2 = Config.C2_INIT\n",
    "\n",
    "    def evaluate_particle(self, position):\n",
    "        n_layers = int(np.clip(np.round(position[0]), Config.BOUNDS['layers_min'], Config.BOUNDS['layers_max']))\n",
    "        n_neurons = int(np.clip(np.round(position[1]), Config.BOUNDS['neurons_min'], Config.BOUNDS['neurons_max']))\n",
    "        \n",
    "        print(f\"    > Eval: L={n_layers}, N={n_neurons}...\", end=\" \")\n",
    "        \n",
    "        model = DynamicLSTM(self.input_dim, n_layers, n_neurons).to(Config.DEVICE)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        model.train()\n",
    "        for _ in range(Config.EPOCHS_EVAL):\n",
    "            for X_b, y_b in self.train_loader:\n",
    "                X_b, y_b = X_b.to(Config.DEVICE), y_b.to(Config.DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(X_b).squeeze(), y_b)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        total_mse, count = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_b, y_b in self.test_loader:\n",
    "                X_b, y_b = X_b.to(Config.DEVICE), y_b.to(Config.DEVICE)\n",
    "                total_mse += criterion(model(X_b).squeeze(), y_b).item() * X_b.size(0)\n",
    "                count += X_b.size(0)\n",
    "        \n",
    "        rmse = math.sqrt(total_mse / count)\n",
    "        print(f\"RMSE: {rmse:.5f}\")\n",
    "        \n",
    "        self.memory.add_record(n_layers, n_neurons, rmse)\n",
    "        return rmse\n",
    "\n",
    "    def run(self):\n",
    "        print(\"\\n=== Starting LLM-Controlled PSO (c1/c2 + Particle History) ===\")\n",
    "        \n",
    "        # Initial evaluation\n",
    "        fitness = np.array([self.evaluate_particle(p) for p in self.positions])\n",
    "        self.pbest_val = fitness.copy()\n",
    "        best_idx = fitness.argmin()\n",
    "        self.gbest_val = fitness[best_idx]\n",
    "        self.gbest_pos = self.positions[best_idx].copy()\n",
    "        \n",
    "        for iteration in range(Config.MAX_ITERATIONS):\n",
    "            # Adaptive inertia\n",
    "            w = Config.W_MAX - ((Config.W_MAX - Config.W_MIN) * iteration / Config.MAX_ITERATIONS)\n",
    "            \n",
    "            print(f\"\\nIter {iteration+1}/{Config.MAX_ITERATIONS} | Best RMSE: {self.gbest_val:.5f} | c1={self.c1:.2f}, c2={self.c2:.2f}\")\n",
    "            \n",
    "            # Store particle state for LLM\n",
    "            self.memory.add_particle_state(iteration, self.positions, self.velocities, fitness)\n",
    "            \n",
    "            # LLM intervention at specific iterations\n",
    "            if iteration in [2, 5, 8]:\n",
    "                mem_ctx = self.memory.get_context_string()\n",
    "                particle_ctx = self.memory.get_particle_state_string()\n",
    "                c1c2_ctx = self.memory.get_c1_c2_history_string()\n",
    "                \n",
    "                suggestions, new_c1, new_c2 = get_llm_suggestions_with_params(\n",
    "                    mem_ctx, particle_ctx, c1c2_ctx, self.c1, self.c2\n",
    "                )\n",
    "                \n",
    "                # Update c1, c2 from LLM\n",
    "                if new_c1 != self.c1 or new_c2 != self.c2:\n",
    "                    print(f\"  [LLM] Updating: c1={self.c1:.2f}->{new_c1:.2f}, c2={self.c2:.2f}->{new_c2:.2f}\")\n",
    "                    self.c1, self.c2 = new_c1, new_c2\n",
    "                    self.memory.add_c1_c2_record(self.c1, self.c2, iteration)\n",
    "                \n",
    "                # Apply suggestions\n",
    "                if suggestions:\n",
    "                    worst_indices = np.argsort(fitness)[-len(suggestions):]\n",
    "                    for idx, sug in zip(worst_indices, suggestions):\n",
    "                        print(f\"  [LLM] Replacing P{idx} -> {sug}\")\n",
    "                        self.positions[idx] = np.array(sug)\n",
    "                        self.velocities[idx] = np.zeros(2)\n",
    "                        new_fit = self.evaluate_particle(self.positions[idx])\n",
    "                        fitness[idx] = new_fit\n",
    "                        if new_fit < self.gbest_val:\n",
    "                            self.gbest_val = new_fit\n",
    "                            self.gbest_pos = self.positions[idx].copy()\n",
    "                            print(f\"  *** LLM FOUND NEW GLOBAL BEST! ***\")\n",
    "            \n",
    "            # PSO update\n",
    "            for i in range(Config.POPULATION_SIZE):\n",
    "                r1, r2 = np.random.rand(2), np.random.rand(2)\n",
    "                \n",
    "                # Cognitive-only for last particle\n",
    "                social_term = 0 if i == Config.POPULATION_SIZE - 1 else self.c2 * r2 * (self.gbest_pos - self.positions[i])\n",
    "                cognitive_term = self.c1 * r1 * (self.pbest_pos[i] - self.positions[i])\n",
    "                \n",
    "                self.velocities[i] = w * self.velocities[i] + cognitive_term + social_term\n",
    "                self.positions[i] += self.velocities[i]\n",
    "                \n",
    "                # Bounds\n",
    "                self.positions[i, 0] = np.clip(self.positions[i, 0], Config.BOUNDS['layers_min'], Config.BOUNDS['layers_max'])\n",
    "                self.positions[i, 1] = np.clip(self.positions[i, 1], Config.BOUNDS['neurons_min'], Config.BOUNDS['neurons_max'])\n",
    "                \n",
    "                fit = self.evaluate_particle(self.positions[i])\n",
    "                fitness[i] = fit\n",
    "                \n",
    "                if fit < self.pbest_val[i]:\n",
    "                    self.pbest_val[i] = fit\n",
    "                    self.pbest_pos[i] = self.positions[i].copy()\n",
    "                if fit < self.gbest_val:\n",
    "                    self.gbest_val = fit\n",
    "                    self.gbest_pos = self.positions[i].copy()\n",
    "        \n",
    "        return self.gbest_pos, self.gbest_val\n",
    "\n",
    "# ==========================================\n",
    "# 7. MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TASK 1: LLM Controls c1/c2 + Particle Position/Velocity History\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    train_loader, test_loader, input_dim, _ = get_data()\n",
    "    if train_loader is None:\n",
    "        return\n",
    "    \n",
    "    start = time.time()\n",
    "    pso = LLMControlledPSO(train_loader, test_loader, input_dim)\n",
    "    best_pos, best_rmse = pso.run()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"RESULT: Best Params = [L={int(best_pos[0])}, N={int(best_pos[1])}]\")\n",
    "    print(f\"        Best RMSE = {best_rmse:.5f}\")\n",
    "    print(f\"        Time = {elapsed:.1f}s\")\n",
    "    print(f\"        Final c1={pso.c1:.2f}, c2={pso.c2:.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T20:58:45.565428Z",
     "iopub.status.busy": "2025-12-14T20:58:45.565209Z",
     "iopub.status.idle": "2025-12-14T21:16:34.874273Z",
     "shell.execute_reply": "2025-12-14T21:16:34.873480Z",
     "shell.execute_reply.started": "2025-12-14T20:58:45.565411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 2: Expanded 5D Search Space (Layers, Neurons, LR, Dropout, Epochs)\n",
      "================================================================================\n",
      "Downloading dataset...\n",
      "\n",
      "----------------------------------------\n",
      "Running VANILLA 5D PSO...\n",
      "\n",
      "=== Starting 5D Vanilla PSO ===\n",
      "    > Eval: L=4, N=72, LR=0.0043, Drop=0.12, Ep=1... RMSE: 0.02218\n",
      "    > Eval: L=2, N=35, LR=0.0031, Drop=0.47, Ep=1... RMSE: 0.02977\n",
      "    > Eval: L=2, N=70, LR=0.0014, Drop=0.41, Ep=2... RMSE: 0.02708\n",
      "    > Eval: L=5, N=219, LR=0.0054, Drop=0.08, Ep=4... RMSE: 0.02381\n",
      "    > Eval: L=4, N=58, LR=0.0089, Drop=0.45, Ep=4... RMSE: 0.02341\n",
      "\n",
      "Iter 1/10 | Best RMSE: 0.02218\n",
      "    > Eval: L=4, N=72, LR=0.0043, Drop=0.12, Ep=1... RMSE: 0.02710\n",
      "    > Eval: L=3, N=58, LR=0.0033, Drop=0.21, Ep=1... RMSE: 0.02801\n",
      "    > Eval: L=2, N=71, LR=0.0041, Drop=0.04, Ep=2... RMSE: 0.02265\n",
      "    > Eval: L=5, N=16, LR=0.0053, Drop=0.12, Ep=3... RMSE: 0.02357\n",
      "    > Eval: L=4, N=58, LR=0.0089, Drop=0.45, Ep=4... RMSE: 0.02481\n",
      "\n",
      "Iter 2/10 | Best RMSE: 0.02218\n",
      "    > Eval: L=4, N=72, LR=0.0043, Drop=0.12, Ep=1... RMSE: 0.02320\n",
      "    > Eval: L=5, N=102, LR=0.0044, Drop=0.00, Ep=2... RMSE: 0.02584\n",
      "    > Eval: L=4, N=72, LR=0.0066, Drop=0.00, Ep=1... RMSE: 0.02579\n",
      "    > Eval: L=3, N=16, LR=0.0050, Drop=0.16, Ep=2... RMSE: 0.02635\n",
      "    > Eval: L=4, N=58, LR=0.0089, Drop=0.45, Ep=4... RMSE: 0.02314\n",
      "\n",
      "Iter 3/10 | Best RMSE: 0.02218\n",
      "    > Eval: L=4, N=72, LR=0.0043, Drop=0.12, Ep=1... RMSE: 0.02404\n",
      "    > Eval: L=6, N=132, LR=0.0053, Drop=0.00, Ep=2... RMSE: 0.02213\n",
      "    > Eval: L=4, N=140, LR=0.0040, Drop=0.00, Ep=2... RMSE: 0.02195\n",
      "    > Eval: L=3, N=111, LR=0.0032, Drop=0.00, Ep=4... RMSE: 0.02208\n",
      "    > Eval: L=4, N=58, LR=0.0089, Drop=0.45, Ep=4... RMSE: 0.02443\n",
      "\n",
      "Iter 4/10 | Best RMSE: 0.02195\n",
      "    > Eval: L=4, N=115, LR=0.0038, Drop=0.04, Ep=2... RMSE: 0.02391\n",
      "    > Eval: L=6, N=169, LR=0.0038, Drop=0.00, Ep=2... RMSE: 0.02160\n",
      "    > Eval: L=6, N=193, LR=0.0019, Drop=0.00, Ep=3... RMSE: 0.02360\n",
      "    > Eval: L=4, N=256, LR=0.0029, Drop=0.00, Ep=3... RMSE: 0.02190\n",
      "    > Eval: L=4, N=58, LR=0.0089, Drop=0.45, Ep=4... RMSE: 0.02406\n",
      "\n",
      "Iter 5/10 | Best RMSE: 0.02160\n",
      "    > Eval: L=4, N=200, LR=0.0036, Drop=0.06, Ep=1... RMSE: 0.02251\n",
      "    > Eval: L=5, N=194, LR=0.0028, Drop=0.00, Ep=2... RMSE: 0.02298\n",
      "    > Eval: L=2, N=114, LR=0.0060, Drop=0.00, Ep=3... RMSE: 0.02207\n",
      "    > Eval: L=5, N=219, LR=0.0027, Drop=0.00, Ep=2... RMSE: 0.02233\n",
      "    > Eval: L=4, N=58, LR=0.0089, Drop=0.45, Ep=4... RMSE: 0.02297\n",
      "\n",
      "Iter 6/10 | Best RMSE: 0.02160\n",
      "    > Eval: L=6, N=16, LR=0.0046, Drop=0.15, Ep=2... RMSE: 0.02300\n",
      "    > Eval: L=6, N=163, LR=0.0047, Drop=0.00, Ep=2... RMSE: 0.02261\n",
      "    > Eval: L=4, N=149, LR=0.0035, Drop=0.00, Ep=2... RMSE: 0.02271\n",
      "    > Eval: L=6, N=177, LR=0.0038, Drop=0.00, Ep=2... RMSE: 0.09464\n",
      "    > Eval: L=4, N=58, LR=0.0089, Drop=0.45, Ep=4... RMSE: 0.02336\n",
      "\n",
      "Iter 7/10 | Best RMSE: 0.02160\n",
      "    > Eval: L=3, N=38, LR=0.0036, Drop=0.05, Ep=1... RMSE: 0.02259\n",
      "    > Eval: L=6, N=149, LR=0.0045, Drop=0.00, Ep=2... RMSE: 0.02206\n",
      "    > Eval: L=6, N=182, LR=0.0025, Drop=0.00, Ep=2... RMSE: 0.02121\n",
      "    > Eval: L=5, N=256, LR=0.0025, Drop=0.00, Ep=3... RMSE: 0.02151\n",
      "    > Eval: L=4, N=58, LR=0.0089, Drop=0.45, Ep=4... RMSE: 0.02332\n",
      "\n",
      "Iter 8/10 | Best RMSE: 0.02121\n",
      "    > Eval: L=5, N=256, LR=0.0038, Drop=0.00, Ep=1... RMSE: 0.02301\n",
      "    > Eval: L=6, N=177, LR=0.0021, Drop=0.00, Ep=2... RMSE: 0.02193\n",
      "    > Eval: L=6, N=201, LR=0.0019, Drop=0.00, Ep=2... RMSE: 0.02765\n",
      "    > Eval: L=6, N=256, LR=0.0017, Drop=0.00, Ep=2... RMSE: 0.02215\n",
      "    > Eval: L=4, N=58, LR=0.0089, Drop=0.45, Ep=4... RMSE: 0.02404\n",
      "\n",
      "Iter 9/10 | Best RMSE: 0.02121\n",
      "    > Eval: L=6, N=21, LR=0.0028, Drop=0.14, Ep=2... RMSE: 0.02370\n",
      "    > Eval: L=6, N=184, LR=0.0023, Drop=0.00, Ep=2... RMSE: 0.02372\n",
      "    > Eval: L=6, N=174, LR=0.0023, Drop=0.00, Ep=2... RMSE: 0.02490\n",
      "    > Eval: L=5, N=138, LR=0.0029, Drop=0.00, Ep=3... RMSE: 0.02409\n",
      "    > Eval: L=4, N=58, LR=0.0089, Drop=0.45, Ep=4... RMSE: 0.02264\n",
      "\n",
      "Iter 10/10 | Best RMSE: 0.02121\n",
      "    > Eval: L=3, N=188, LR=0.0022, Drop=0.13, Ep=2... RMSE: 0.02393\n",
      "    > Eval: L=6, N=160, LR=0.0051, Drop=0.00, Ep=2... RMSE: 0.02297\n",
      "    > Eval: L=6, N=179, LR=0.0026, Drop=0.00, Ep=2... RMSE: 0.02280\n",
      "    > Eval: L=6, N=256, LR=0.0022, Drop=0.00, Ep=3... RMSE: 0.02181\n",
      "    > Eval: L=4, N=58, LR=0.0089, Drop=0.45, Ep=4... RMSE: 0.02158\n",
      "\n",
      "----------------------------------------\n",
      "Running LLM-ENHANCED 5D PSO...\n",
      "\n",
      "=== Starting 5D LLM-Enhanced PSO ===\n",
      "    > Eval: L=2, N=155, LR=0.0062, Drop=0.25, Ep=2... RMSE: 0.02163\n",
      "    > Eval: L=4, N=250, LR=0.0034, Drop=0.47, Ep=2... RMSE: 0.02459\n",
      "    > Eval: L=6, N=102, LR=0.0055, Drop=0.13, Ep=4... RMSE: 0.02341\n",
      "    > Eval: L=5, N=253, LR=0.0048, Drop=0.40, Ep=5... RMSE: 0.02307\n",
      "    > Eval: L=5, N=127, LR=0.0007, Drop=0.35, Ep=5... RMSE: 0.02257\n",
      "\n",
      "Iter 1/10 | Best RMSE: 0.02163\n",
      "    > Eval: L=2, N=155, LR=0.0062, Drop=0.25, Ep=2... RMSE: 0.02164\n",
      "    > Eval: L=1, N=164, LR=0.0086, Drop=0.03, Ep=2... RMSE: 0.02220\n",
      "    > Eval: L=4, N=128, LR=0.0060, Drop=0.19, Ep=3... RMSE: 0.02144\n",
      "    > Eval: L=4, N=16, LR=0.0052, Drop=0.13, Ep=1... RMSE: 0.02695\n",
      "    > Eval: L=5, N=127, LR=0.0007, Drop=0.35, Ep=5... RMSE: 0.02161\n",
      "\n",
      "Iter 2/10 | Best RMSE: 0.02144\n",
      "    > Eval: L=4, N=106, LR=0.0058, Drop=0.14, Ep=2... RMSE: 0.02592\n",
      "    > Eval: L=1, N=67, LR=0.0100, Drop=0.00, Ep=2... RMSE: 0.02236\n",
      "    > Eval: L=3, N=150, LR=0.0064, Drop=0.23, Ep=1... RMSE: 0.02325\n",
      "    > Eval: L=5, N=16, LR=0.0058, Drop=0.30, Ep=1... RMSE: 0.02812\n",
      "    > Eval: L=5, N=127, LR=0.0007, Drop=0.35, Ep=5... RMSE: 0.02155\n",
      "\n",
      "Iter 3/10 | Best RMSE: 0.02144\n",
      "  [LLM] Consulting for 5D suggestions...\n",
      "  [LLM Analysis]: Best configs cluster around 4–5 layers, ~125–155 neurons, LR either quite low (~7e-4) or mid (~6e-3), dropout ~0.19–0.35, epochs 3–5. Worst configs share small neuron counts (16–106) and low epochs (1–2). This suggests: (1) avoid very small networks and 1 epoch; (2) deeper models benefit from lower LR and slightly higher dropout; (3) mid-depth models can use a somewhat higher LR with moderate dropout. I propose one exploitative config near the current best but with slightly more capacity and lower LR, and one exploratory config with fewer layers but more neurons and a mid LR.\n",
      "  [LLM] Replacing P0 -> [5, 160, 0.0005, 0.32, 5]\n",
      "    > Eval: L=5, N=160, LR=0.0005, Drop=0.32, Ep=5... RMSE: 0.02177\n",
      "  [LLM] Replacing P3 -> [3, 192, 0.0045, 0.22, 4]\n",
      "    > Eval: L=3, N=192, LR=0.0045, Drop=0.22, Ep=4... RMSE: 0.02264\n",
      "    > Eval: L=3, N=118, LR=0.0100, Drop=0.22, Ep=1... RMSE: 0.02389\n",
      "    > Eval: L=1, N=191, LR=0.0099, Drop=0.25, Ep=3... RMSE: 0.02277\n",
      "    > Eval: L=5, N=131, LR=0.0054, Drop=0.15, Ep=3... RMSE: 0.02242\n",
      "    > Eval: L=5, N=105, LR=0.0070, Drop=0.39, Ep=3... RMSE: 0.03223\n",
      "    > Eval: L=5, N=127, LR=0.0007, Drop=0.35, Ep=5... RMSE: 0.02223\n",
      "\n",
      "Iter 4/10 | Best RMSE: 0.02144\n",
      "    > Eval: L=2, N=111, LR=0.0100, Drop=0.11, Ep=1... RMSE: 0.02295\n",
      "    > Eval: L=1, N=126, LR=0.0013, Drop=0.35, Ep=1... RMSE: 0.02925\n",
      "    > Eval: L=5, N=112, LR=0.0060, Drop=0.12, Ep=3... RMSE: 0.02558\n",
      "    > Eval: L=6, N=256, LR=0.0073, Drop=0.17, Ep=5... RMSE: 0.10218\n",
      "    > Eval: L=5, N=127, LR=0.0007, Drop=0.35, Ep=5... RMSE: 0.02411\n",
      "\n",
      "Iter 5/10 | Best RMSE: 0.02144\n",
      "    > Eval: L=4, N=187, LR=0.0100, Drop=0.10, Ep=2... RMSE: 0.09704\n",
      "    > Eval: L=6, N=145, LR=0.0007, Drop=0.02, Ep=2... RMSE: 0.02189\n",
      "    > Eval: L=3, N=105, LR=0.0063, Drop=0.19, Ep=2... RMSE: 0.02483\n",
      "    > Eval: L=4, N=256, LR=0.0029, Drop=0.44, Ep=2... RMSE: 0.02445\n",
      "    > Eval: L=5, N=127, LR=0.0007, Drop=0.35, Ep=5... RMSE: 0.02389\n",
      "\n",
      "Iter 6/10 | Best RMSE: 0.02144\n",
      "  [LLM] Consulting for 5D suggestions...\n",
      "  [LLM Analysis]: Best configs cluster around 4–5 layers, ~127–155 neurons, mid-to-low LR (0.0007–0.0062), moderate dropout (0.19–0.35), and 2–5 epochs. Worst configs suggest that high LR with deeper/wider networks (e.g., LR≈0.01 or 0.0073 with many layers/neurons) is harmful. Also, extreme capacity (6 layers, 256 neurons) with relatively high LR performs poorly. I’ll exploit near the current best (slightly lower LR, modest tweak to depth/neurons/dropout) and explore a slightly shallower but wider model with conservative LR and dropout.\n",
      "  [LLM] Replacing P2 -> [4, 140, 0.0015, 0.22, 4]\n",
      "    > Eval: L=4, N=140, LR=0.0015, Drop=0.22, Ep=4... RMSE: 0.02301\n",
      "  [LLM] Replacing P0 -> [3, 180, 0.0009, 0.28, 4]\n",
      "    > Eval: L=3, N=180, LR=0.0009, Drop=0.28, Ep=4... RMSE: 0.02215\n",
      "    > Eval: L=5, N=117, LR=0.0100, Drop=0.11, Ep=1... RMSE: 0.02478\n",
      "    > Eval: L=6, N=125, LR=0.0017, Drop=0.00, Ep=3... RMSE: 0.02380\n",
      "    > Eval: L=4, N=136, LR=0.0100, Drop=0.19, Ep=3... RMSE: 0.09936\n",
      "    > Eval: L=4, N=108, LR=0.0039, Drop=0.39, Ep=4... RMSE: 0.02156\n",
      "    > Eval: L=5, N=127, LR=0.0007, Drop=0.35, Ep=5... RMSE: 0.02151\n",
      "\n",
      "Iter 7/10 | Best RMSE: 0.02144\n",
      "    > Eval: L=1, N=129, LR=0.0100, Drop=0.14, Ep=1... RMSE: 0.02251\n",
      "    > Eval: L=6, N=130, LR=0.0025, Drop=0.37, Ep=2... RMSE: 0.02884\n",
      "    > Eval: L=4, N=129, LR=0.0100, Drop=0.16, Ep=2... RMSE: 0.04411\n",
      "    > Eval: L=4, N=54, LR=0.0057, Drop=0.11, Ep=3... RMSE: 0.02342\n",
      "    > Eval: L=5, N=127, LR=0.0007, Drop=0.35, Ep=5... RMSE: 0.02193\n",
      "\n",
      "Iter 8/10 | Best RMSE: 0.02144\n",
      "    > Eval: L=4, N=142, LR=0.0032, Drop=0.20, Ep=1... RMSE: 0.02321\n",
      "    > Eval: L=6, N=139, LR=0.0031, Drop=0.00, Ep=3... RMSE: 0.02146\n",
      "    > Eval: L=4, N=123, LR=0.0001, Drop=0.19, Ep=3... RMSE: 0.04100\n",
      "    > Eval: L=4, N=207, LR=0.0038, Drop=0.50, Ep=4... RMSE: 0.02220\n",
      "    > Eval: L=5, N=127, LR=0.0007, Drop=0.35, Ep=5... RMSE: 0.02171\n",
      "\n",
      "Iter 9/10 | Best RMSE: 0.02144\n",
      "  [LLM] Consulting for 5D suggestions...\n",
      "  [LLM Analysis]: Best configs cluster around 4–6 layers, ~110–140 neurons, LR in the 0.0007–0.006 range, and moderate dropout (0.19–0.39). Worst configs clearly involve high LR (≈0.01), suggesting sensitivity to learning rate, especially with deeper/wider models. Also, higher epochs (4–5) appear in good configs when LR is small. I’ll exploit near the current best (4–5 layers, 110–140 neurons, LR ~0.002–0.004, dropout ~0.15–0.30, epochs 3–4) and explore a slightly deeper, smaller-neuron, lower-LR model with more epochs for stability.\n",
      "  [LLM] Replacing P0 -> [4, 120, 0.0035, 0.22, 4]\n",
      "    > Eval: L=4, N=120, LR=0.0035, Drop=0.22, Ep=4... RMSE: 0.02152\n",
      "  [LLM] Replacing P2 -> [5, 96, 0.0015, 0.3, 5]\n",
      "    > Eval: L=5, N=96, LR=0.0015, Drop=0.30, Ep=5... RMSE: 0.02224\n",
      "    > Eval: L=2, N=170, LR=0.0100, Drop=0.20, Ep=1... RMSE: 0.02371\n",
      "    > Eval: L=4, N=129, LR=0.0085, Drop=0.00, Ep=3... RMSE: 0.02681\n",
      "    > Eval: L=2, N=177, LR=0.0100, Drop=0.00, Ep=1... RMSE: 0.02258\n",
      "    > Eval: L=4, N=81, LR=0.0069, Drop=0.26, Ep=3... RMSE: 0.02263\n",
      "    > Eval: L=5, N=127, LR=0.0007, Drop=0.35, Ep=5... RMSE: 0.02173\n",
      "\n",
      "Iter 10/10 | Best RMSE: 0.02144\n",
      "    > Eval: L=2, N=156, LR=0.0100, Drop=0.25, Ep=1... RMSE: 0.02341\n",
      "    > Eval: L=4, N=139, LR=0.0001, Drop=0.00, Ep=2... RMSE: 0.05160\n",
      "    > Eval: L=6, N=129, LR=0.0095, Drop=0.16, Ep=1... RMSE: 0.09460\n",
      "    > Eval: L=3, N=61, LR=0.0032, Drop=0.06, Ep=3... RMSE: 0.02253\n",
      "    > Eval: L=5, N=127, LR=0.0007, Drop=0.35, Ep=5... RMSE: 0.02150\n",
      "\n",
      "================================================================================\n",
      "METHOD               | LAYERS | NEURONS | LR       | DROP  | EPOCHS | RMSE       | TIME\n",
      "--------------------------------------------------------------------------------\n",
      "Vanilla 5D PSO       | 6      | 182     | 0.0025   | 0.00  | 2      | 0.02121    | 462.5s\n",
      "LLM-Enhanced 5D      | 4      | 128     | 0.0060   | 0.19  | 3      | 0.02144    | 605.3s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# PSO_regression_task2.py\n",
    "# Task 2: Expand search space to include learning_rate, dropout, epochs\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (EXPANDED SEARCH SPACE)\n",
    "# ==========================================\n",
    "class Config:\n",
    "    AVALAI_API_KEY = \"\"\n",
    "    AVALAI_BASE_URL = \"https://api.avalai.ir/v1/chat/completions\"\n",
    "    LLM_MODEL = \"gpt-5.1\"\n",
    "\n",
    "    DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv\"\n",
    "    SEQ_LENGTH = 24\n",
    "    TRAIN_SPLIT = 0.8\n",
    "    BATCH_SIZE = 128\n",
    "\n",
    "    POPULATION_SIZE = 5\n",
    "    MAX_ITERATIONS = 10\n",
    "    C1 = 2.0\n",
    "    C2 = 2.0\n",
    "    W_MAX = 0.9\n",
    "    W_MIN = 0.4\n",
    "\n",
    "    # EXPANDED SEARCH SPACE (5 dimensions now!)\n",
    "    BOUNDS = {\n",
    "        'layers_min': 1, 'layers_max': 6,\n",
    "        'neurons_min': 16, 'neurons_max': 256,\n",
    "        'lr_min': 0.0001, 'lr_max': 0.01,        # Learning rate\n",
    "        'dropout_min': 0.0, 'dropout_max': 0.5,   # Dropout rate\n",
    "        'epochs_min': 1, 'epochs_max': 5          # Training epochs\n",
    "    }\n",
    "    \n",
    "    # Dimension indices\n",
    "    DIM_LAYERS = 0\n",
    "    DIM_NEURONS = 1\n",
    "    DIM_LR = 2\n",
    "    DIM_DROPOUT = 3\n",
    "    DIM_EPOCHS = 4\n",
    "    NUM_DIMS = 5\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. MEMORY MODULE\n",
    "# ==========================================\n",
    "class OptimizationMemory:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "    \n",
    "    def add_record(self, params, rmse):\n",
    "        \"\"\"params = [layers, neurons, lr, dropout, epochs]\"\"\"\n",
    "        self.history.append({'params': params, 'rmse': float(rmse)})\n",
    "    \n",
    "    def get_context_string(self):\n",
    "        if not self.history:\n",
    "            return \"No history yet.\"\n",
    "        \n",
    "        sorted_hist = sorted(self.history, key=lambda x: x['rmse'])\n",
    "        best_5 = sorted_hist[:5]\n",
    "        worst_3 = sorted_hist[-3:]\n",
    "        \n",
    "        context = \"=== OPTIMIZATION MEMORY (5D Search Space) ===\\n\"\n",
    "        context += \"TOP 5 Configurations:\\n\"\n",
    "        for i, h in enumerate(best_5):\n",
    "            p = h['params']\n",
    "            context += f\"  {i+1}. [L={p[0]}, N={p[1]}, LR={p[2]:.4f}, Drop={p[3]:.2f}, Ep={p[4]}] -> RMSE: {h['rmse']:.5f}\\n\"\n",
    "        \n",
    "        context += \"\\nWORST 3 Configurations:\\n\"\n",
    "        for i, h in enumerate(worst_3):\n",
    "            p = h['params']\n",
    "            context += f\"  {i+1}. [L={p[0]}, N={p[1]}, LR={p[2]:.4f}, Drop={p[3]:.2f}, Ep={p[4]}] -> RMSE: {h['rmse']:.5f}\\n\"\n",
    "        \n",
    "        return context\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "def get_data():\n",
    "    print(\"Downloading dataset...\")\n",
    "    try:\n",
    "        df = pd.read_csv(Config.DATASET_URL)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    cols = ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']\n",
    "    df = df[cols]\n",
    "    df['pm2.5'] = df['pm2.5'].ffill().bfill()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    values = df.values.astype('float32')\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled) - Config.SEQ_LENGTH):\n",
    "        X.append(scaled[i:i+Config.SEQ_LENGTH, :])\n",
    "        y.append(scaled[i+Config.SEQ_LENGTH, 0])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    split = int(len(X) * Config.TRAIN_SPLIT)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(torch.from_numpy(X[:split]), torch.from_numpy(y[:split])), \n",
    "                              batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(torch.from_numpy(X[split:]), torch.from_numpy(y[split:])), \n",
    "                             batch_size=Config.BATCH_SIZE)\n",
    "    \n",
    "    return train_loader, test_loader, X.shape[2], scaler\n",
    "\n",
    "# ==========================================\n",
    "# 4. MODEL (with configurable dropout)\n",
    "# ==========================================\n",
    "class DynamicLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers, hidden_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = int(num_layers)\n",
    "        self.hidden_size = int(hidden_size)\n",
    "        self.dropout_rate = float(dropout_rate)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, self.hidden_size, self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout_rate if self.num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        self.fc = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(Config.DEVICE)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(Config.DEVICE)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        return self.fc(out)\n",
    "\n",
    "# ==========================================\n",
    "# 5. LLM INTERFACE (5D suggestions)\n",
    "# ==========================================\n",
    "def get_llm_suggestions_5d(memory_context):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in PSO Hyperparameter Optimization for LSTM models.\n",
    "\n",
    "OBJECTIVE: Minimize RMSE.\n",
    "\n",
    "SEARCH SPACE (5 Dimensions):\n",
    "1. Layers: {Config.BOUNDS['layers_min']} to {Config.BOUNDS['layers_max']} (integer)\n",
    "2. Neurons: {Config.BOUNDS['neurons_min']} to {Config.BOUNDS['neurons_max']} (integer)\n",
    "3. Learning Rate: {Config.BOUNDS['lr_min']} to {Config.BOUNDS['lr_max']} (float)\n",
    "4. Dropout: {Config.BOUNDS['dropout_min']} to {Config.BOUNDS['dropout_max']} (float)\n",
    "5. Epochs: {Config.BOUNDS['epochs_min']} to {Config.BOUNDS['epochs_max']} (integer)\n",
    "\n",
    "{memory_context}\n",
    "\n",
    "TASK: Suggest 2 new configurations based on the patterns you observe.\n",
    "- Consider interactions between parameters (e.g., more layers may need lower LR).\n",
    "- Balance exploration and exploitation.\n",
    "\n",
    "OUTPUT FORMAT (JSON only):\n",
    "{{\n",
    "    \"analysis\": \"Brief reasoning...\",\n",
    "    \"suggestions\": [\n",
    "        [layers1, neurons1, lr1, dropout1, epochs1],\n",
    "        [layers2, neurons2, lr2, dropout2, epochs2]\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"  [LLM] Consulting for 5D suggestions...\")\n",
    "        resp = requests.post(\n",
    "            Config.AVALAI_BASE_URL,\n",
    "            headers={\"Authorization\": f\"Bearer {Config.AVALAI_API_KEY}\", \"Content-Type\": \"application/json\"},\n",
    "            json={\"model\": Config.LLM_MODEL, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.6},\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            content = resp.json()['choices'][0]['message']['content']\n",
    "            content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            data = json.loads(content)\n",
    "            print(f\"  [LLM Analysis]: {data.get('analysis', 'N/A')}\")\n",
    "            return data.get(\"suggestions\", [])\n",
    "        else:\n",
    "            print(f\"  [LLM Error] Status: {resp.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"  [LLM Failed]: {e}\")\n",
    "        return []\n",
    "\n",
    "# ==========================================\n",
    "# 6. 5D PSO\n",
    "# ==========================================\n",
    "class ExpandedPSO:\n",
    "    def __init__(self, train_loader, test_loader, input_dim, use_llm=True):\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.input_dim = input_dim\n",
    "        self.use_llm = use_llm\n",
    "        self.memory = OptimizationMemory()\n",
    "        \n",
    "        # Initialize 5D swarm\n",
    "        self.positions = np.zeros((Config.POPULATION_SIZE, Config.NUM_DIMS))\n",
    "        self.positions[:, Config.DIM_LAYERS] = np.random.uniform(Config.BOUNDS['layers_min'], Config.BOUNDS['layers_max'], Config.POPULATION_SIZE)\n",
    "        self.positions[:, Config.DIM_NEURONS] = np.random.uniform(Config.BOUNDS['neurons_min'], Config.BOUNDS['neurons_max'], Config.POPULATION_SIZE)\n",
    "        self.positions[:, Config.DIM_LR] = np.random.uniform(Config.BOUNDS['lr_min'], Config.BOUNDS['lr_max'], Config.POPULATION_SIZE)\n",
    "        self.positions[:, Config.DIM_DROPOUT] = np.random.uniform(Config.BOUNDS['dropout_min'], Config.BOUNDS['dropout_max'], Config.POPULATION_SIZE)\n",
    "        self.positions[:, Config.DIM_EPOCHS] = np.random.uniform(Config.BOUNDS['epochs_min'], Config.BOUNDS['epochs_max'], Config.POPULATION_SIZE)\n",
    "        \n",
    "        self.velocities = np.zeros((Config.POPULATION_SIZE, Config.NUM_DIMS))\n",
    "        self.pbest_pos = self.positions.copy()\n",
    "        self.pbest_val = np.full(Config.POPULATION_SIZE, float('inf'))\n",
    "        self.gbest_pos = np.zeros(Config.NUM_DIMS)\n",
    "        self.gbest_val = float('inf')\n",
    "\n",
    "    def clip_position(self, pos):\n",
    "        \"\"\"Clip all dimensions to valid ranges\"\"\"\n",
    "        pos[Config.DIM_LAYERS] = np.clip(pos[Config.DIM_LAYERS], Config.BOUNDS['layers_min'], Config.BOUNDS['layers_max'])\n",
    "        pos[Config.DIM_NEURONS] = np.clip(pos[Config.DIM_NEURONS], Config.BOUNDS['neurons_min'], Config.BOUNDS['neurons_max'])\n",
    "        pos[Config.DIM_LR] = np.clip(pos[Config.DIM_LR], Config.BOUNDS['lr_min'], Config.BOUNDS['lr_max'])\n",
    "        pos[Config.DIM_DROPOUT] = np.clip(pos[Config.DIM_DROPOUT], Config.BOUNDS['dropout_min'], Config.BOUNDS['dropout_max'])\n",
    "        pos[Config.DIM_EPOCHS] = np.clip(pos[Config.DIM_EPOCHS], Config.BOUNDS['epochs_min'], Config.BOUNDS['epochs_max'])\n",
    "        return pos\n",
    "\n",
    "    def decode_position(self, pos):\n",
    "        \"\"\"Convert continuous position to actual hyperparameters\"\"\"\n",
    "        return {\n",
    "            'layers': int(np.round(pos[Config.DIM_LAYERS])),\n",
    "            'neurons': int(np.round(pos[Config.DIM_NEURONS])),\n",
    "            'lr': float(pos[Config.DIM_LR]),\n",
    "            'dropout': float(pos[Config.DIM_DROPOUT]),\n",
    "            'epochs': int(np.round(pos[Config.DIM_EPOCHS]))\n",
    "        }\n",
    "\n",
    "    def evaluate_particle(self, position):\n",
    "        params = self.decode_position(position)\n",
    "        \n",
    "        print(f\"    > Eval: L={params['layers']}, N={params['neurons']}, LR={params['lr']:.4f}, Drop={params['dropout']:.2f}, Ep={params['epochs']}...\", end=\" \")\n",
    "        \n",
    "        try:\n",
    "            model = DynamicLSTM(self.input_dim, params['layers'], params['neurons'], params['dropout']).to(Config.DEVICE)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "            \n",
    "            model.train()\n",
    "            for _ in range(params['epochs']):\n",
    "                for X_b, y_b in self.train_loader:\n",
    "                    X_b, y_b = X_b.to(Config.DEVICE), y_b.to(Config.DEVICE)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = criterion(model(X_b).squeeze(), y_b)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            total_mse, count = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for X_b, y_b in self.test_loader:\n",
    "                    X_b, y_b = X_b.to(Config.DEVICE), y_b.to(Config.DEVICE)\n",
    "                    total_mse += criterion(model(X_b).squeeze(), y_b).item() * X_b.size(0)\n",
    "                    count += X_b.size(0)\n",
    "            \n",
    "            rmse = math.sqrt(total_mse / count)\n",
    "            print(f\"RMSE: {rmse:.5f}\")\n",
    "            \n",
    "            self.memory.add_record([params['layers'], params['neurons'], params['lr'], params['dropout'], params['epochs']], rmse)\n",
    "            return rmse\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "    def run(self):\n",
    "        mode = \"LLM-Enhanced\" if self.use_llm else \"Vanilla\"\n",
    "        print(f\"\\n=== Starting 5D {mode} PSO ===\")\n",
    "        \n",
    "        # Initial evaluation\n",
    "        fitness = np.array([self.evaluate_particle(p) for p in self.positions])\n",
    "        self.pbest_val = fitness.copy()\n",
    "        best_idx = fitness.argmin()\n",
    "        self.gbest_val = fitness[best_idx]\n",
    "        self.gbest_pos = self.positions[best_idx].copy()\n",
    "        \n",
    "        for iteration in range(Config.MAX_ITERATIONS):\n",
    "            w = Config.W_MAX - ((Config.W_MAX - Config.W_MIN) * iteration / Config.MAX_ITERATIONS)\n",
    "            print(f\"\\nIter {iteration+1}/{Config.MAX_ITERATIONS} | Best RMSE: {self.gbest_val:.5f}\")\n",
    "            \n",
    "            # LLM intervention\n",
    "            if self.use_llm and iteration in [2, 5, 8]:\n",
    "                suggestions = get_llm_suggestions_5d(self.memory.get_context_string())\n",
    "                if suggestions:\n",
    "                    worst_indices = np.argsort(fitness)[-len(suggestions):]\n",
    "                    for idx, sug in zip(worst_indices, suggestions):\n",
    "                        print(f\"  [LLM] Replacing P{idx} -> {sug}\")\n",
    "                        self.positions[idx] = np.array(sug)\n",
    "                        self.velocities[idx] = np.zeros(Config.NUM_DIMS)\n",
    "                        new_fit = self.evaluate_particle(self.positions[idx])\n",
    "                        fitness[idx] = new_fit\n",
    "                        if new_fit < self.gbest_val:\n",
    "                            self.gbest_val = new_fit\n",
    "                            self.gbest_pos = self.positions[idx].copy()\n",
    "                            print(f\"  *** LLM FOUND NEW GLOBAL BEST! ***\")\n",
    "            \n",
    "            # PSO update\n",
    "            for i in range(Config.POPULATION_SIZE):\n",
    "                r1 = np.random.rand(Config.NUM_DIMS)\n",
    "                r2 = np.random.rand(Config.NUM_DIMS)\n",
    "                \n",
    "                social_term = 0 if i == Config.POPULATION_SIZE - 1 else Config.C2 * r2 * (self.gbest_pos - self.positions[i])\n",
    "                cognitive_term = Config.C1 * r1 * (self.pbest_pos[i] - self.positions[i])\n",
    "                \n",
    "                self.velocities[i] = w * self.velocities[i] + cognitive_term + social_term\n",
    "                self.positions[i] += self.velocities[i]\n",
    "                self.positions[i] = self.clip_position(self.positions[i])\n",
    "                \n",
    "                fit = self.evaluate_particle(self.positions[i])\n",
    "                fitness[i] = fit\n",
    "                \n",
    "                if fit < self.pbest_val[i]:\n",
    "                    self.pbest_val[i] = fit\n",
    "                    self.pbest_pos[i] = self.positions[i].copy()\n",
    "                if fit < self.gbest_val:\n",
    "                    self.gbest_val = fit\n",
    "                    self.gbest_pos = self.positions[i].copy()\n",
    "        \n",
    "        return self.gbest_pos, self.gbest_val\n",
    "\n",
    "# ==========================================\n",
    "# 7. MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TASK 2: Expanded 5D Search Space (Layers, Neurons, LR, Dropout, Epochs)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    train_loader, test_loader, input_dim, _ = get_data()\n",
    "    if train_loader is None:\n",
    "        return\n",
    "    \n",
    "    # Run Vanilla 5D PSO\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Running VANILLA 5D PSO...\")\n",
    "    start = time.time()\n",
    "    pso_v = ExpandedPSO(train_loader, test_loader, input_dim, use_llm=False)\n",
    "    best_v, rmse_v = pso_v.run()\n",
    "    time_v = time.time() - start\n",
    "    \n",
    "    # Run LLM-Enhanced 5D PSO\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Running LLM-ENHANCED 5D PSO...\")\n",
    "    start = time.time()\n",
    "    pso_l = ExpandedPSO(train_loader, test_loader, input_dim, use_llm=True)\n",
    "    best_l, rmse_l = pso_l.run()\n",
    "    time_l = time.time() - start\n",
    "    \n",
    "    # Results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{'METHOD':<20} | {'LAYERS':<6} | {'NEURONS':<7} | {'LR':<8} | {'DROP':<5} | {'EPOCHS':<6} | {'RMSE':<10} | {'TIME'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    v_params = pso_v.decode_position(best_v)\n",
    "    l_params = pso_l.decode_position(best_l)\n",
    "    \n",
    "    print(f\"{'Vanilla 5D PSO':<20} | {v_params['layers']:<6} | {v_params['neurons']:<7} | {v_params['lr']:<8.4f} | {v_params['dropout']:<5.2f} | {v_params['epochs']:<6} | {rmse_v:<10.5f} | {time_v:.1f}s\")\n",
    "    print(f\"{'LLM-Enhanced 5D':<20} | {l_params['layers']:<6} | {l_params['neurons']:<7} | {l_params['lr']:<8.4f} | {l_params['dropout']:<5.2f} | {l_params['epochs']:<6} | {rmse_l:<10.5f} | {time_l:.1f}s\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:16:34.875932Z",
     "iopub.status.busy": "2025-12-14T21:16:34.875711Z",
     "iopub.status.idle": "2025-12-14T21:46:50.587729Z",
     "shell.execute_reply": "2025-12-14T21:46:50.586976Z",
     "shell.execute_reply.started": "2025-12-14T21:16:34.875914Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 3: Compare Inertia Weight Strategies from Paper\n",
      "Based on: 'A novel PSO algorithm with adaptive inertia weight'\n",
      "================================================================================\n",
      "Downloading dataset...\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== Starting PSO with W Strategy: LINEAR_DECREASE ===\n",
      "    > Eval: L=4, N=238... RMSE: 0.02224\n",
      "    > Eval: L=3, N=249... RMSE: 0.02391\n",
      "    > Eval: L=4, N=117... RMSE: 0.02249\n",
      "    > Eval: L=2, N=27... RMSE: 0.03350\n",
      "    > Eval: L=1, N=46... RMSE: 0.02451\n",
      "\n",
      "Iter 1/10 | Best RMSE: 0.02224 | W=0.900 | Success Rate=1.00\n",
      "    > Eval: L=4, N=238... RMSE: 0.02156\n",
      "    > Eval: L=4, N=241... RMSE: 0.02345\n",
      "    > Eval: L=3, N=256... RMSE: 0.02299\n",
      "    > Eval: L=5, N=234... RMSE: 0.02214\n",
      "    > Eval: L=1, N=46... RMSE: 0.02916\n",
      "\n",
      "Iter 2/10 | Best RMSE: 0.02156 | W=0.850 | Success Rate=0.00\n",
      "    > Eval: L=4, N=238... RMSE: 0.02295\n",
      "    > Eval: L=4, N=231... RMSE: 0.02190\n",
      "    > Eval: L=5, N=256... RMSE: 0.02199\n",
      "    > Eval: L=6, N=256... RMSE: 0.02190\n",
      "    > Eval: L=1, N=46... RMSE: 0.02406\n",
      "\n",
      "Iter 3/10 | Best RMSE: 0.02156 | W=0.800 | Success Rate=0.00\n",
      "    > Eval: L=4, N=238... RMSE: 0.02201\n",
      "    > Eval: L=5, N=237... RMSE: 0.02294\n",
      "    > Eval: L=5, N=256... RMSE: 0.02203\n",
      "    > Eval: L=6, N=256... RMSE: 0.02374\n",
      "    > Eval: L=1, N=46... RMSE: 0.02606\n",
      "\n",
      "Iter 4/10 | Best RMSE: 0.02156 | W=0.750 | Success Rate=0.00\n",
      "    > Eval: L=4, N=238... RMSE: 0.02230\n",
      "    > Eval: L=4, N=235... RMSE: 0.02296\n",
      "    > Eval: L=4, N=256... RMSE: 0.02209\n",
      "    > Eval: L=5, N=256... RMSE: 0.02225\n",
      "    > Eval: L=1, N=46... RMSE: 0.02501\n",
      "\n",
      "Iter 5/10 | Best RMSE: 0.02156 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=4, N=238... RMSE: 0.02169\n",
      "    > Eval: L=4, N=232... RMSE: 0.02197\n",
      "    > Eval: L=3, N=256... RMSE: 0.02158\n",
      "    > Eval: L=4, N=256... RMSE: 0.02261\n",
      "    > Eval: L=1, N=46... RMSE: 0.02454\n",
      "\n",
      "Iter 6/10 | Best RMSE: 0.02156 | W=0.650 | Success Rate=0.00\n",
      "    > Eval: L=4, N=238... RMSE: 0.02411\n",
      "    > Eval: L=4, N=238... RMSE: 0.02262\n",
      "    > Eval: L=3, N=238... RMSE: 0.02364\n",
      "    > Eval: L=6, N=256... RMSE: 0.02460\n",
      "    > Eval: L=1, N=46... RMSE: 0.02556\n",
      "\n",
      "Iter 7/10 | Best RMSE: 0.02156 | W=0.600 | Success Rate=0.00\n",
      "    > Eval: L=4, N=238... RMSE: 0.02495\n",
      "    > Eval: L=4, N=240... RMSE: 0.02289\n",
      "    > Eval: L=4, N=232... RMSE: 0.02241\n",
      "    > Eval: L=5, N=244... RMSE: 0.02577\n",
      "    > Eval: L=1, N=46... RMSE: 0.02698\n",
      "\n",
      "Iter 8/10 | Best RMSE: 0.02156 | W=0.550 | Success Rate=0.00\n",
      "    > Eval: L=4, N=238... RMSE: 0.02177\n",
      "    > Eval: L=4, N=225... RMSE: 0.02474\n",
      "    > Eval: L=4, N=256... RMSE: 0.02212\n",
      "    > Eval: L=2, N=231... RMSE: 0.02164\n",
      "    > Eval: L=1, N=46... RMSE: 0.02573\n",
      "\n",
      "Iter 9/10 | Best RMSE: 0.02156 | W=0.500 | Success Rate=0.00\n",
      "    > Eval: L=4, N=238... RMSE: 0.02627\n",
      "    > Eval: L=4, N=248... RMSE: 0.02779\n",
      "    > Eval: L=3, N=256... RMSE: 0.02691\n",
      "    > Eval: L=1, N=230... RMSE: 0.02469\n",
      "    > Eval: L=1, N=46... RMSE: 0.02531\n",
      "\n",
      "Iter 10/10 | Best RMSE: 0.02156 | W=0.450 | Success Rate=0.00\n",
      "    > Eval: L=4, N=238... RMSE: 0.02337\n",
      "    > Eval: L=4, N=230... RMSE: 0.02474\n",
      "    > Eval: L=3, N=256... RMSE: 0.02258\n",
      "    > Eval: L=4, N=240... RMSE: 0.02167\n",
      "    > Eval: L=1, N=46... RMSE: 0.02768\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== Starting PSO with W Strategy: LINEAR_INCREASE ===\n",
      "    > Eval: L=3, N=229... RMSE: 0.02399\n",
      "    > Eval: L=1, N=166... RMSE: 0.02258\n",
      "    > Eval: L=5, N=191... RMSE: 0.02668\n",
      "    > Eval: L=6, N=97... RMSE: 0.02341\n",
      "    > Eval: L=5, N=243... RMSE: 0.02673\n",
      "\n",
      "Iter 1/10 | Best RMSE: 0.02258 | W=0.400 | Success Rate=1.00\n",
      "    > Eval: L=1, N=142... RMSE: 0.02359\n",
      "    > Eval: L=1, N=166... RMSE: 0.02415\n",
      "    > Eval: L=4, N=148... RMSE: 0.02403\n",
      "    > Eval: L=1, N=175... RMSE: 0.02314\n",
      "    > Eval: L=5, N=243... RMSE: 0.02261\n",
      "\n",
      "Iter 2/10 | Best RMSE: 0.02258 | W=0.450 | Success Rate=0.00\n",
      "    > Eval: L=1, N=112... RMSE: 0.02514\n",
      "    > Eval: L=1, N=166... RMSE: 0.02395\n",
      "    > Eval: L=4, N=138... RMSE: 0.02670\n",
      "    > Eval: L=1, N=206... RMSE: 0.02504\n",
      "    > Eval: L=5, N=243... RMSE: 0.02180\n",
      "\n",
      "Iter 3/10 | Best RMSE: 0.02180 | W=0.500 | Success Rate=0.00\n",
      "    > Eval: L=6, N=256... RMSE: 0.02330\n",
      "    > Eval: L=6, N=197... RMSE: 0.02431\n",
      "    > Eval: L=6, N=256... RMSE: 0.02190\n",
      "    > Eval: L=6, N=250... RMSE: 0.02610\n",
      "    > Eval: L=5, N=243... RMSE: 0.02653\n",
      "\n",
      "Iter 4/10 | Best RMSE: 0.02180 | W=0.550 | Success Rate=0.00\n",
      "    > Eval: L=6, N=256... RMSE: 0.02612\n",
      "    > Eval: L=3, N=168... RMSE: 0.02287\n",
      "    > Eval: L=6, N=256... RMSE: 0.02579\n",
      "    > Eval: L=1, N=151... RMSE: 0.02272\n",
      "    > Eval: L=5, N=243... RMSE: 0.02190\n",
      "\n",
      "Iter 5/10 | Best RMSE: 0.02180 | W=0.600 | Success Rate=0.00\n",
      "    > Eval: L=6, N=256... RMSE: 0.02265\n",
      "    > Eval: L=3, N=221... RMSE: 0.02164\n",
      "    > Eval: L=6, N=256... RMSE: 0.02232\n",
      "    > Eval: L=1, N=209... RMSE: 0.02335\n",
      "    > Eval: L=5, N=243... RMSE: 0.02209\n",
      "\n",
      "Iter 6/10 | Best RMSE: 0.02164 | W=0.650 | Success Rate=0.00\n",
      "    > Eval: L=1, N=256... RMSE: 0.02184\n",
      "    > Eval: L=3, N=255... RMSE: 0.02292\n",
      "    > Eval: L=2, N=252... RMSE: 0.02388\n",
      "    > Eval: L=2, N=139... RMSE: 0.02204\n",
      "    > Eval: L=5, N=243... RMSE: 0.02232\n",
      "\n",
      "Iter 7/10 | Best RMSE: 0.02164 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=1, N=219... RMSE: 0.02421\n",
      "    > Eval: L=3, N=195... RMSE: 0.02474\n",
      "    > Eval: L=1, N=238... RMSE: 0.02307\n",
      "    > Eval: L=3, N=226... RMSE: 0.02161\n",
      "    > Eval: L=5, N=243... RMSE: 0.02169\n",
      "\n",
      "Iter 8/10 | Best RMSE: 0.02161 | W=0.750 | Success Rate=0.00\n",
      "    > Eval: L=1, N=256... RMSE: 0.02171\n",
      "    > Eval: L=3, N=183... RMSE: 0.02218\n",
      "    > Eval: L=3, N=239... RMSE: 0.02617\n",
      "    > Eval: L=3, N=256... RMSE: 0.02266\n",
      "    > Eval: L=5, N=243... RMSE: 0.02592\n",
      "\n",
      "Iter 9/10 | Best RMSE: 0.02161 | W=0.800 | Success Rate=0.00\n",
      "    > Eval: L=2, N=256... RMSE: 0.02355\n",
      "    > Eval: L=2, N=235... RMSE: 0.02151\n",
      "    > Eval: L=6, N=256... RMSE: 0.02318\n",
      "    > Eval: L=2, N=256... RMSE: 0.02204\n",
      "    > Eval: L=5, N=243... RMSE: 0.02281\n",
      "\n",
      "Iter 10/10 | Best RMSE: 0.02151 | W=0.850 | Success Rate=0.00\n",
      "    > Eval: L=3, N=245... RMSE: 0.02398\n",
      "    > Eval: L=2, N=256... RMSE: 0.02158\n",
      "    > Eval: L=4, N=237... RMSE: 0.02261\n",
      "    > Eval: L=2, N=221... RMSE: 0.02347\n",
      "    > Eval: L=5, N=243... RMSE: 0.02449\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== Starting PSO with W Strategy: RANDOM ===\n",
      "    > Eval: L=2, N=162... RMSE: 0.02221\n",
      "    > Eval: L=3, N=56... RMSE: 0.02563\n",
      "    > Eval: L=6, N=20... RMSE: 0.03121\n",
      "    > Eval: L=3, N=173... RMSE: 0.02266\n",
      "    > Eval: L=4, N=34... RMSE: 0.02824\n",
      "\n",
      "Iter 1/10 | Best RMSE: 0.02221 | W=0.753 | Success Rate=1.00\n",
      "    > Eval: L=2, N=162... RMSE: 0.02265\n",
      "    > Eval: L=2, N=59... RMSE: 0.02336\n",
      "    > Eval: L=1, N=256... RMSE: 0.02173\n",
      "    > Eval: L=1, N=219... RMSE: 0.02297\n",
      "    > Eval: L=4, N=34... RMSE: 0.02981\n",
      "\n",
      "Iter 2/10 | Best RMSE: 0.02173 | W=0.864 | Success Rate=0.00\n",
      "    > Eval: L=1, N=231... RMSE: 0.02336\n",
      "    > Eval: L=1, N=256... RMSE: 0.02233\n",
      "    > Eval: L=1, N=256... RMSE: 0.02591\n",
      "    > Eval: L=2, N=256... RMSE: 0.02210\n",
      "    > Eval: L=4, N=34... RMSE: 0.02971\n",
      "\n",
      "Iter 3/10 | Best RMSE: 0.02173 | W=0.571 | Success Rate=0.00\n",
      "    > Eval: L=2, N=189... RMSE: 0.02165\n",
      "    > Eval: L=2, N=256... RMSE: 0.02330\n",
      "    > Eval: L=1, N=256... RMSE: 0.02247\n",
      "    > Eval: L=2, N=196... RMSE: 0.02265\n",
      "    > Eval: L=4, N=34... RMSE: 0.02994\n",
      "\n",
      "Iter 4/10 | Best RMSE: 0.02165 | W=0.832 | Success Rate=0.00\n",
      "    > Eval: L=3, N=155... RMSE: 0.02397\n",
      "    > Eval: L=2, N=256... RMSE: 0.02201\n",
      "    > Eval: L=1, N=256... RMSE: 0.02260\n",
      "    > Eval: L=2, N=136... RMSE: 0.02238\n",
      "    > Eval: L=4, N=34... RMSE: 0.02838\n",
      "\n",
      "Iter 5/10 | Best RMSE: 0.02165 | W=0.594 | Success Rate=0.00\n",
      "    > Eval: L=2, N=202... RMSE: 0.02158\n",
      "    > Eval: L=3, N=250... RMSE: 0.02335\n",
      "    > Eval: L=1, N=253... RMSE: 0.02541\n",
      "    > Eval: L=2, N=256... RMSE: 0.02147\n",
      "    > Eval: L=4, N=34... RMSE: 0.02865\n",
      "\n",
      "Iter 6/10 | Best RMSE: 0.02147 | W=0.737 | Success Rate=0.00\n",
      "    > Eval: L=2, N=256... RMSE: 0.02436\n",
      "    > Eval: L=1, N=256... RMSE: 0.02299\n",
      "    > Eval: L=2, N=256... RMSE: 0.02184\n",
      "    > Eval: L=1, N=256... RMSE: 0.02179\n",
      "    > Eval: L=4, N=34... RMSE: 0.02837\n",
      "\n",
      "Iter 7/10 | Best RMSE: 0.02147 | W=0.680 | Success Rate=0.00\n",
      "    > Eval: L=2, N=256... RMSE: 0.02212\n",
      "    > Eval: L=2, N=256... RMSE: 0.02271\n",
      "    > Eval: L=2, N=256... RMSE: 0.02201\n",
      "    > Eval: L=2, N=256... RMSE: 0.02242\n",
      "    > Eval: L=4, N=34... RMSE: 0.02912\n",
      "\n",
      "Iter 8/10 | Best RMSE: 0.02147 | W=0.698 | Success Rate=0.00\n",
      "    > Eval: L=2, N=233... RMSE: 0.02546\n",
      "    > Eval: L=3, N=256... RMSE: 0.02368\n",
      "    > Eval: L=1, N=256... RMSE: 0.02408\n",
      "    > Eval: L=2, N=256... RMSE: 0.02488\n",
      "    > Eval: L=4, N=34... RMSE: 0.03218\n",
      "\n",
      "Iter 9/10 | Best RMSE: 0.02147 | W=0.818 | Success Rate=0.00\n",
      "    > Eval: L=1, N=216... RMSE: 0.02476\n",
      "    > Eval: L=3, N=256... RMSE: 0.02185\n",
      "    > Eval: L=1, N=256... RMSE: 0.02149\n",
      "    > Eval: L=2, N=256... RMSE: 0.02185\n",
      "    > Eval: L=4, N=34... RMSE: 0.03220\n",
      "\n",
      "Iter 10/10 | Best RMSE: 0.02147 | W=0.615 | Success Rate=0.00\n",
      "    > Eval: L=3, N=222... RMSE: 0.02394\n",
      "    > Eval: L=1, N=256... RMSE: 0.02262\n",
      "    > Eval: L=2, N=256... RMSE: 0.02273\n",
      "    > Eval: L=2, N=256... RMSE: 0.02464\n",
      "    > Eval: L=4, N=34... RMSE: 0.02831\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== Starting PSO with W Strategy: SUCCESS_RATE ===\n",
      "    > Eval: L=6, N=191... RMSE: 0.02189\n",
      "    > Eval: L=5, N=232... RMSE: 0.02255\n",
      "    > Eval: L=2, N=25... RMSE: 0.02846\n",
      "    > Eval: L=5, N=39... RMSE: 0.02863\n",
      "    > Eval: L=3, N=216... RMSE: 0.02554\n",
      "\n",
      "Iter 1/10 | Best RMSE: 0.02189 | W=0.000 | Success Rate=1.00\n",
      "    > Eval: L=6, N=191... RMSE: 0.02299\n",
      "    > Eval: L=6, N=219... RMSE: 0.02245\n",
      "    > Eval: L=3, N=256... RMSE: 0.02238\n",
      "    > Eval: L=6, N=156... RMSE: 0.02232\n",
      "    > Eval: L=3, N=216... RMSE: 0.02510\n",
      "\n",
      "Iter 2/10 | Best RMSE: 0.02189 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=6, N=191... RMSE: 0.02291\n",
      "    > Eval: L=6, N=164... RMSE: 0.02256\n",
      "    > Eval: L=6, N=129... RMSE: 0.02233\n",
      "    > Eval: L=6, N=198... RMSE: 0.02190\n",
      "    > Eval: L=3, N=216... RMSE: 0.02473\n",
      "\n",
      "Iter 3/10 | Best RMSE: 0.02189 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=6, N=191... RMSE: 0.02170\n",
      "    > Eval: L=6, N=225... RMSE: 0.02334\n",
      "    > Eval: L=6, N=165... RMSE: 0.02524\n",
      "    > Eval: L=6, N=184... RMSE: 0.02276\n",
      "    > Eval: L=3, N=216... RMSE: 0.02417\n",
      "\n",
      "Iter 4/10 | Best RMSE: 0.02170 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=6, N=191... RMSE: 0.02260\n",
      "    > Eval: L=6, N=199... RMSE: 0.02239\n",
      "    > Eval: L=6, N=116... RMSE: 0.02289\n",
      "    > Eval: L=6, N=199... RMSE: 0.02249\n",
      "    > Eval: L=3, N=216... RMSE: 0.02281\n",
      "\n",
      "Iter 5/10 | Best RMSE: 0.02170 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=6, N=191... RMSE: 0.02621\n",
      "    > Eval: L=6, N=193... RMSE: 0.02314\n",
      "    > Eval: L=6, N=238... RMSE: 0.02179\n",
      "    > Eval: L=6, N=189... RMSE: 0.02529\n",
      "    > Eval: L=3, N=216... RMSE: 0.02334\n",
      "\n",
      "Iter 6/10 | Best RMSE: 0.02170 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=6, N=191... RMSE: 0.02198\n",
      "    > Eval: L=6, N=202... RMSE: 0.02171\n",
      "    > Eval: L=6, N=190... RMSE: 0.02266\n",
      "    > Eval: L=6, N=193... RMSE: 0.02469\n",
      "    > Eval: L=3, N=216... RMSE: 0.02226\n",
      "\n",
      "Iter 7/10 | Best RMSE: 0.02170 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=6, N=191... RMSE: 0.02589\n",
      "    > Eval: L=6, N=190... RMSE: 0.02344\n",
      "    > Eval: L=6, N=226... RMSE: 0.02434\n",
      "    > Eval: L=6, N=198... RMSE: 0.03493\n",
      "    > Eval: L=3, N=216... RMSE: 0.02273\n",
      "\n",
      "Iter 8/10 | Best RMSE: 0.02170 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=6, N=191... RMSE: 0.02682\n",
      "    > Eval: L=6, N=194... RMSE: 0.02191\n",
      "    > Eval: L=6, N=196... RMSE: 0.02314\n",
      "    > Eval: L=6, N=188... RMSE: 0.02286\n",
      "    > Eval: L=3, N=216... RMSE: 0.02296\n",
      "\n",
      "Iter 9/10 | Best RMSE: 0.02170 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=6, N=191... RMSE: 0.02139\n",
      "    > Eval: L=6, N=199... RMSE: 0.02208\n",
      "    > Eval: L=6, N=237... RMSE: 0.02289\n",
      "    > Eval: L=6, N=200... RMSE: 0.02189\n",
      "    > Eval: L=3, N=216... RMSE: 0.02550\n",
      "\n",
      "Iter 10/10 | Best RMSE: 0.02139 | W=0.000 | Success Rate=0.00\n",
      "    > Eval: L=6, N=191... RMSE: 0.02174\n",
      "    > Eval: L=6, N=199... RMSE: 0.02648\n",
      "    > Eval: L=6, N=149... RMSE: 0.02223\n",
      "    > Eval: L=6, N=195... RMSE: 0.02299\n",
      "    > Eval: L=3, N=216... RMSE: 0.02205\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== Starting PSO with W Strategy: RANK_BASED ===\n",
      "    > Eval: L=3, N=252... RMSE: 0.02201\n",
      "    > Eval: L=2, N=126... RMSE: 0.02167\n",
      "    > Eval: L=3, N=58... RMSE: 0.02725\n",
      "    > Eval: L=4, N=81... RMSE: 0.02345\n",
      "    > Eval: L=2, N=231... RMSE: 0.02239\n",
      "\n",
      "Iter 1/10 | Best RMSE: 0.02167 | W=0.700 | Success Rate=1.00\n",
      "    > Eval: L=3, N=151... RMSE: 0.02320\n",
      "    > Eval: L=2, N=126... RMSE: 0.02355\n",
      "    > Eval: L=1, N=72... RMSE: 0.02488\n",
      "    > Eval: L=3, N=104... RMSE: 0.02368\n",
      "    > Eval: L=2, N=231... RMSE: 0.02195\n",
      "\n",
      "Iter 2/10 | Best RMSE: 0.02167 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=3, N=111... RMSE: 0.02238\n",
      "    > Eval: L=2, N=126... RMSE: 0.02265\n",
      "    > Eval: L=1, N=97... RMSE: 0.02657\n",
      "    > Eval: L=1, N=114... RMSE: 0.02286\n",
      "    > Eval: L=2, N=231... RMSE: 0.02147\n",
      "\n",
      "Iter 3/10 | Best RMSE: 0.02147 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=2, N=256... RMSE: 0.02349\n",
      "    > Eval: L=2, N=182... RMSE: 0.02345\n",
      "    > Eval: L=2, N=201... RMSE: 0.02390\n",
      "    > Eval: L=2, N=256... RMSE: 0.02185\n",
      "    > Eval: L=2, N=231... RMSE: 0.02215\n",
      "\n",
      "Iter 4/10 | Best RMSE: 0.02147 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=3, N=256... RMSE: 0.02229\n",
      "    > Eval: L=2, N=231... RMSE: 0.02204\n",
      "    > Eval: L=3, N=256... RMSE: 0.02431\n",
      "    > Eval: L=3, N=256... RMSE: 0.02503\n",
      "    > Eval: L=2, N=231... RMSE: 0.02247\n",
      "\n",
      "Iter 5/10 | Best RMSE: 0.02147 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=3, N=256... RMSE: 0.02244\n",
      "    > Eval: L=2, N=244... RMSE: 0.02656\n",
      "    > Eval: L=2, N=256... RMSE: 0.02226\n",
      "    > Eval: L=2, N=256... RMSE: 0.02236\n",
      "    > Eval: L=2, N=231... RMSE: 0.02264\n",
      "\n",
      "Iter 6/10 | Best RMSE: 0.02147 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=2, N=256... RMSE: 0.02582\n",
      "    > Eval: L=2, N=105... RMSE: 0.02482\n",
      "    > Eval: L=3, N=256... RMSE: 0.02247\n",
      "    > Eval: L=2, N=242... RMSE: 0.02149\n",
      "    > Eval: L=2, N=231... RMSE: 0.02197\n",
      "\n",
      "Iter 7/10 | Best RMSE: 0.02147 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=3, N=236... RMSE: 0.02298\n",
      "    > Eval: L=2, N=152... RMSE: 0.02218\n",
      "    > Eval: L=3, N=216... RMSE: 0.02198\n",
      "    > Eval: L=3, N=235... RMSE: 0.02241\n",
      "    > Eval: L=2, N=231... RMSE: 0.02141\n",
      "\n",
      "Iter 8/10 | Best RMSE: 0.02141 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=3, N=227... RMSE: 0.02298\n",
      "    > Eval: L=2, N=256... RMSE: 0.02162\n",
      "    > Eval: L=2, N=196... RMSE: 0.02294\n",
      "    > Eval: L=1, N=230... RMSE: 0.02240\n",
      "    > Eval: L=2, N=231... RMSE: 0.02398\n",
      "\n",
      "Iter 9/10 | Best RMSE: 0.02141 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=3, N=256... RMSE: 0.02207\n",
      "    > Eval: L=3, N=256... RMSE: 0.02290\n",
      "    > Eval: L=2, N=212... RMSE: 0.02380\n",
      "    > Eval: L=1, N=234... RMSE: 0.02270\n",
      "    > Eval: L=2, N=231... RMSE: 0.02237\n",
      "\n",
      "Iter 10/10 | Best RMSE: 0.02141 | W=0.700 | Success Rate=0.00\n",
      "    > Eval: L=3, N=219... RMSE: 0.02409\n",
      "    > Eval: L=2, N=256... RMSE: 0.02427\n",
      "    > Eval: L=3, N=243... RMSE: 0.02272\n",
      "    > Eval: L=3, N=246... RMSE: 0.02404\n",
      "    > Eval: L=2, N=231... RMSE: 0.02206\n",
      "\n",
      "====================================================================================================\n",
      "COMPARISON OF INERTIA WEIGHT STRATEGIES\n",
      "====================================================================================================\n",
      "STRATEGY             | LAYERS | NEURONS | RMSE       | TIME(s)  | AVG W   | W STD\n",
      "----------------------------------------------------------------------------------------------------\n",
      "linear_decrease      | 4      | 238     | 0.02156    | 324.7    | 0.675   | 0.144\n",
      "linear_increase      | 2      | 235     | 0.02151    | 376.2    | 0.625   | 0.144\n",
      "random               | 2      | 256     | 0.02147    | 192.0    | 0.716   | 0.098\n",
      "success_rate         | 6      | 191     | 0.02139    | 699.0    | 0.000   | 0.000\n",
      "rank_based           | 2      | 231     | 0.02141    | 221.3    | 0.700   | 0.000\n",
      "====================================================================================================\n",
      "\n",
      "🏆 BEST STRATEGY: SUCCESS_RATE\n",
      "   Best RMSE: 0.02139\n",
      "   Parameters: L=6, N=191\n"
     ]
    }
   ],
   "source": [
    "# PSO_regression_task3.py\n",
    "# Task 3: Implement AIWPSO (Success Rate based W) + Rank-based W from paper\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    AVALAI_API_KEY = \"\"\n",
    "    AVALAI_BASE_URL = \"https://api.avalai.ir/v1/chat/completions\"\n",
    "    LLM_MODEL = \"gpt-5.1\"\n",
    "\n",
    "    DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv\"\n",
    "    SEQ_LENGTH = 24\n",
    "    TRAIN_SPLIT = 0.8\n",
    "    BATCH_SIZE = 128\n",
    "\n",
    "    POPULATION_SIZE = 5\n",
    "    MAX_ITERATIONS = 10\n",
    "    C1 = 2.0\n",
    "    C2 = 2.0\n",
    "    W_MAX = 1.0  # Paper uses [0, 1.0] range\n",
    "    W_MIN = 0.0\n",
    "\n",
    "    BOUNDS = {\n",
    "        'layers_min': 1, 'layers_max': 6,\n",
    "        'neurons_min': 16, 'neurons_max': 256\n",
    "    }\n",
    "\n",
    "    EPOCHS_EVAL = 2\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA LOADING\n",
    "# ==========================================\n",
    "def get_data():\n",
    "    print(\"Downloading dataset...\")\n",
    "    try:\n",
    "        df = pd.read_csv(Config.DATASET_URL)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    cols = ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']\n",
    "    df = df[cols]\n",
    "    df['pm2.5'] = df['pm2.5'].ffill().bfill()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    values = df.values.astype('float32')\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled) - Config.SEQ_LENGTH):\n",
    "        X.append(scaled[i:i+Config.SEQ_LENGTH, :])\n",
    "        y.append(scaled[i+Config.SEQ_LENGTH, 0])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    split = int(len(X) * Config.TRAIN_SPLIT)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(torch.from_numpy(X[:split]), torch.from_numpy(y[:split])), \n",
    "                              batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(torch.from_numpy(X[split:]), torch.from_numpy(y[split:])), \n",
    "                             batch_size=Config.BATCH_SIZE)\n",
    "    \n",
    "    return train_loader, test_loader, X.shape[2], scaler\n",
    "\n",
    "# ==========================================\n",
    "# 3. MODEL\n",
    "# ==========================================\n",
    "class DynamicLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers, hidden_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = int(num_layers)\n",
    "        self.hidden_size = int(hidden_size)\n",
    "        self.lstm = nn.LSTM(input_dim, self.hidden_size, self.num_layers, \n",
    "                           batch_first=True, dropout=0.2 if self.num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(Config.DEVICE)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(Config.DEVICE)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "# ==========================================\n",
    "# 4. INERTIA WEIGHT STRATEGIES (FROM PAPER)\n",
    "# ==========================================\n",
    "class InertiaWeightStrategy:\n",
    "    \"\"\"\n",
    "    Implements various W strategies from the paper:\n",
    "    \"A novel particle swarm optimization algorithm with adaptive inertia weight\"\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_decreasing(iteration, max_iter, w_max=0.9, w_min=0.4):\n",
    "        \"\"\"W3: Standard linear decrease\"\"\"\n",
    "        return w_max - ((w_max - w_min) * iteration / max_iter)\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_increasing(iteration, max_iter, w_max=0.9, w_min=0.4):\n",
    "        \"\"\"W9: Linear increase (opposite of standard)\"\"\"\n",
    "        return w_min + ((w_max - w_min) * iteration / max_iter)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_inertia():\n",
    "        \"\"\"W2: Random inertia weight\"\"\"\n",
    "        return 0.5 + np.random.rand() / 2  # Range [0.5, 1.0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def success_rate_adaptive(success_rate, w_max=1.0, w_min=0.0):\n",
    "        \"\"\"\n",
    "        AIWPSO (Proposed in paper - Equation 20):\n",
    "        w(t) = (w_max - w_min) * Ps(t) + w_min\n",
    "        \n",
    "        High success rate -> High w (exploration)\n",
    "        Low success rate -> Low w (exploitation)\n",
    "        \"\"\"\n",
    "        return (w_max - w_min) * success_rate + w_min\n",
    "    \n",
    "    @staticmethod\n",
    "    def rank_based(particle_rank, total_population, w_max=0.9, w_min=0.4):\n",
    "        \"\"\"\n",
    "        W13: Rank-based inertia weight (Equation 16)\n",
    "        Better particles get lower w (exploit), worse particles get higher w (explore)\n",
    "        \"\"\"\n",
    "        return w_min + (w_max - w_min) * (particle_rank / total_population)\n",
    "\n",
    "# ==========================================\n",
    "# 5. PSO WITH MULTIPLE W STRATEGIES\n",
    "# ==========================================\n",
    "class AdaptivePSO:\n",
    "    def __init__(self, train_loader, test_loader, input_dim, w_strategy='linear_decrease'):\n",
    "        \"\"\"\n",
    "        w_strategy options:\n",
    "        - 'linear_decrease': Standard decreasing (baseline)\n",
    "        - 'linear_increase': Increasing W\n",
    "        - 'random': Random W each iteration\n",
    "        - 'success_rate': AIWPSO (paper's proposed method)\n",
    "        - 'rank_based': Different W per particle based on rank\n",
    "        \"\"\"\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.input_dim = input_dim\n",
    "        self.w_strategy = w_strategy\n",
    "        \n",
    "        # Initialize swarm\n",
    "        self.positions = np.zeros((Config.POPULATION_SIZE, 2))\n",
    "        self.positions[:, 0] = np.random.uniform(Config.BOUNDS['layers_min'], Config.BOUNDS['layers_max'], Config.POPULATION_SIZE)\n",
    "        self.positions[:, 1] = np.random.uniform(Config.BOUNDS['neurons_min'], Config.BOUNDS['neurons_max'], Config.POPULATION_SIZE)\n",
    "        \n",
    "        self.velocities = np.zeros((Config.POPULATION_SIZE, 2))\n",
    "        self.pbest_pos = self.positions.copy()\n",
    "        self.pbest_val = np.full(Config.POPULATION_SIZE, float('inf'))\n",
    "        self.gbest_pos = np.zeros(2)\n",
    "        self.gbest_val = float('inf')\n",
    "        \n",
    "        # For success rate calculation\n",
    "        self.prev_pbest_val = np.full(Config.POPULATION_SIZE, float('inf'))\n",
    "        \n",
    "        # Track W values for analysis\n",
    "        self.w_history = []\n",
    "\n",
    "    def evaluate_particle(self, position):\n",
    "        n_layers = int(np.clip(np.round(position[0]), Config.BOUNDS['layers_min'], Config.BOUNDS['layers_max']))\n",
    "        n_neurons = int(np.clip(np.round(position[1]), Config.BOUNDS['neurons_min'], Config.BOUNDS['neurons_max']))\n",
    "        \n",
    "        print(f\"    > Eval: L={n_layers}, N={n_neurons}...\", end=\" \")\n",
    "        \n",
    "        model = DynamicLSTM(self.input_dim, n_layers, n_neurons).to(Config.DEVICE)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        model.train()\n",
    "        for _ in range(Config.EPOCHS_EVAL):\n",
    "            for X_b, y_b in self.train_loader:\n",
    "                X_b, y_b = X_b.to(Config.DEVICE), y_b.to(Config.DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(X_b).squeeze(), y_b)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        total_mse, count = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_b, y_b in self.test_loader:\n",
    "                X_b, y_b = X_b.to(Config.DEVICE), y_b.to(Config.DEVICE)\n",
    "                total_mse += criterion(model(X_b).squeeze(), y_b).item() * X_b.size(0)\n",
    "                count += X_b.size(0)\n",
    "        \n",
    "        rmse = math.sqrt(total_mse / count)\n",
    "        print(f\"RMSE: {rmse:.5f}\")\n",
    "        return rmse\n",
    "\n",
    "    def calculate_success_rate(self):\n",
    "        \"\"\"\n",
    "        Equation 17-18 from paper:\n",
    "        Success = 1 if pbest improved, 0 otherwise\n",
    "        Ps = sum(Success) / n\n",
    "        \"\"\"\n",
    "        successes = 0\n",
    "        for i in range(Config.POPULATION_SIZE):\n",
    "            if self.pbest_val[i] < self.prev_pbest_val[i]:\n",
    "                successes += 1\n",
    "        return successes / Config.POPULATION_SIZE\n",
    "\n",
    "    def get_inertia_weight(self, iteration, particle_idx=None, fitness=None):\n",
    "        \"\"\"Get W based on selected strategy\"\"\"\n",
    "        if self.w_strategy == 'linear_decrease':\n",
    "            return InertiaWeightStrategy.linear_decreasing(iteration, Config.MAX_ITERATIONS)\n",
    "        \n",
    "        elif self.w_strategy == 'linear_increase':\n",
    "            return InertiaWeightStrategy.linear_increasing(iteration, Config.MAX_ITERATIONS)\n",
    "        \n",
    "        elif self.w_strategy == 'random':\n",
    "            return InertiaWeightStrategy.random_inertia()\n",
    "        \n",
    "        elif self.w_strategy == 'success_rate':\n",
    "            success_rate = self.calculate_success_rate()\n",
    "            return InertiaWeightStrategy.success_rate_adaptive(success_rate)\n",
    "        \n",
    "        elif self.w_strategy == 'rank_based':\n",
    "            if fitness is not None and particle_idx is not None:\n",
    "                # Rank particles (0 = best, n-1 = worst)\n",
    "                ranks = np.argsort(np.argsort(fitness))\n",
    "                return InertiaWeightStrategy.rank_based(ranks[particle_idx], Config.POPULATION_SIZE)\n",
    "            return 0.7  # Default\n",
    "        \n",
    "        else:\n",
    "            return 0.7  # Default\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"\\n=== Starting PSO with W Strategy: {self.w_strategy.upper()} ===\")\n",
    "        \n",
    "        # Initial evaluation\n",
    "        fitness = np.array([self.evaluate_particle(p) for p in self.positions])\n",
    "        self.pbest_val = fitness.copy()\n",
    "        self.prev_pbest_val = fitness.copy()\n",
    "        best_idx = fitness.argmin()\n",
    "        self.gbest_val = fitness[best_idx]\n",
    "        self.gbest_pos = self.positions[best_idx].copy()\n",
    "        \n",
    "        for iteration in range(Config.MAX_ITERATIONS):\n",
    "            # Store previous pbest for success rate calculation\n",
    "            self.prev_pbest_val = self.pbest_val.copy()\n",
    "            \n",
    "            # Get base W (for strategies that use single W for all particles)\n",
    "            base_w = self.get_inertia_weight(iteration, fitness=fitness)\n",
    "            \n",
    "            # Calculate success rate for logging\n",
    "            if iteration > 0:\n",
    "                success_rate = self.calculate_success_rate()\n",
    "            else:\n",
    "                success_rate = 1.0\n",
    "            \n",
    "            self.w_history.append(base_w)\n",
    "            \n",
    "            print(f\"\\nIter {iteration+1}/{Config.MAX_ITERATIONS} | Best RMSE: {self.gbest_val:.5f} | W={base_w:.3f} | Success Rate={success_rate:.2f}\")\n",
    "            \n",
    "            # PSO update\n",
    "            for i in range(Config.POPULATION_SIZE):\n",
    "                # Get particle-specific W for rank-based strategy\n",
    "                if self.w_strategy == 'rank_based':\n",
    "                    w = self.get_inertia_weight(iteration, particle_idx=i, fitness=fitness)\n",
    "                else:\n",
    "                    w = base_w\n",
    "                \n",
    "                r1, r2 = np.random.rand(2), np.random.rand(2)\n",
    "                \n",
    "                # Cognitive-only for last particle\n",
    "                social_term = 0 if i == Config.POPULATION_SIZE - 1 else Config.C2 * r2 * (self.gbest_pos - self.positions[i])\n",
    "                cognitive_term = Config.C1 * r1 * (self.pbest_pos[i] - self.positions[i])\n",
    "                \n",
    "                self.velocities[i] = w * self.velocities[i] + cognitive_term + social_term\n",
    "                self.positions[i] += self.velocities[i]\n",
    "                \n",
    "                # Bounds\n",
    "                self.positions[i, 0] = np.clip(self.positions[i, 0], Config.BOUNDS['layers_min'], Config.BOUNDS['layers_max'])\n",
    "                self.positions[i, 1] = np.clip(self.positions[i, 1], Config.BOUNDS['neurons_min'], Config.BOUNDS['neurons_max'])\n",
    "                \n",
    "                fit = self.evaluate_particle(self.positions[i])\n",
    "                fitness[i] = fit\n",
    "                \n",
    "                if fit < self.pbest_val[i]:\n",
    "                    self.pbest_val[i] = fit\n",
    "                    self.pbest_pos[i] = self.positions[i].copy()\n",
    "                if fit < self.gbest_val:\n",
    "                    self.gbest_val = fit\n",
    "                    self.gbest_pos = self.positions[i].copy()\n",
    "        \n",
    "        return self.gbest_pos, self.gbest_val, self.w_history\n",
    "\n",
    "# ==========================================\n",
    "# 6. MAIN - COMPARE ALL STRATEGIES\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TASK 3: Compare Inertia Weight Strategies from Paper\")\n",
    "    print(\"Based on: 'A novel PSO algorithm with adaptive inertia weight'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    train_loader, test_loader, input_dim, _ = get_data()\n",
    "    if train_loader is None:\n",
    "        return\n",
    "    \n",
    "    strategies = [\n",
    "        'linear_decrease',   # W3: Standard baseline\n",
    "        'linear_increase',   # W9: Opposite of standard\n",
    "        'random',            # W2: Random W\n",
    "        'success_rate',      # AIWPSO: Paper's proposed method\n",
    "        'rank_based'         # W13: Per-particle W based on fitness rank\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        start = time.time()\n",
    "        pso = AdaptivePSO(train_loader, test_loader, input_dim, w_strategy=strategy)\n",
    "        best_pos, best_rmse, w_history = pso.run()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        results.append({\n",
    "            'strategy': strategy,\n",
    "            'layers': int(np.round(best_pos[0])),\n",
    "            'neurons': int(np.round(best_pos[1])),\n",
    "            'rmse': best_rmse,\n",
    "            'time': elapsed,\n",
    "            'avg_w': np.mean(w_history),\n",
    "            'w_std': np.std(w_history)\n",
    "        })\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"COMPARISON OF INERTIA WEIGHT STRATEGIES\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'STRATEGY':<20} | {'LAYERS':<6} | {'NEURONS':<7} | {'RMSE':<10} | {'TIME(s)':<8} | {'AVG W':<7} | {'W STD'}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r['strategy']:<20} | {r['layers']:<6} | {r['neurons']:<7} | {r['rmse']:<10.5f} | {r['time']:<8.1f} | {r['avg_w']:<7.3f} | {r['w_std']:.3f}\")\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Find best strategy\n",
    "    best = min(results, key=lambda x: x['rmse'])\n",
    "    print(f\"\\n🏆 BEST STRATEGY: {best['strategy'].upper()}\")\n",
    "    print(f\"   Best RMSE: {best['rmse']:.5f}\")\n",
    "    print(f\"   Parameters: L={best['layers']}, N={best['neurons']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
